{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & 필수 클래스 및 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2Config\n",
    "from tqdm import tqdm_notebook\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import os\n",
    "from korquad_qg.config import QGConfig\n",
    "from korquad_qg.utils import TqdmLoggingHandler\n",
    "from typing import List, NamedTuple, Optional, Tuple\n",
    "from korquad_qg.dataset import MAX_QUESTION_SPACE, MIN_QUESTION_SPACE, QAExample, dynamic_padding_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<s>'\n",
    "EOS = '</s>'\n",
    "MASK = '<mask>'\n",
    "PAD = '<pad>'\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file='Tokenizer/tokenizer.json',\n",
    "            bos_token=BOS, eos_token=EOS, unk_token='<unk>', \n",
    "            pad_token=PAD, mask_token=MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPTDecodingInputType = Tuple[torch.Tensor, torch.Tensor]\n",
    "GPTInputsType = Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "GPTFeaturesType = Tuple[List[int], List[float], List[int]]\n",
    "\n",
    "class QAExample(NamedTuple):\n",
    "    context: str\n",
    "    answer: str\n",
    "    question: Optional[str] = None\n",
    "\n",
    "def load_korquad_dataset(dataset_path: str) -> List[QAExample]:\n",
    "    korquad = [pd.read_csv(dataset_path)]\n",
    "    max_len = 512\n",
    "    examples = []\n",
    "    for document in korquad:\n",
    "        for i in tqdm_notebook(range(len(document))):\n",
    "            if len(TOKENIZER.tokenize(document[\"context\"][i])) + 10 <= max_len:\n",
    "                example = QAExample(document[\"context\"][i], document[\"answers\"][i], document[\"question\"][i])\n",
    "                examples.append(example)\n",
    "        \n",
    "    return examples\n",
    "    \n",
    "def dynamic_padding_collate_fn(features: List[GPTFeaturesType]) -> GPTInputsType:\n",
    "    max_seq_len = max([len(feature[0]) for feature in features])\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "\n",
    "    for feature in features:\n",
    "        padded_input_ids = feature[0] + [0] * (max_seq_len - len(feature[0]))\n",
    "        padded_attention_mask = feature[1] + [0.0] * (max_seq_len - len(feature[1]))\n",
    "        padded_labels = feature[2] + [-100] * (max_seq_len - len(feature[2]))\n",
    "\n",
    "        input_ids.append(padded_input_ids)\n",
    "        attention_mask.append(padded_attention_mask)\n",
    "        labels.append(padded_labels)\n",
    "\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryQGDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        examples: List[QAExample],\n",
    "        tokenizer: SentencePieceBPETokenizer,\n",
    "        max_sequence_length: int,\n",
    "        is_train: bool = True,\n",
    "    ) -> None:\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "        self.sos_token = tokenizer.convert_tokens_to_ids(\"<s>\")\n",
    "        self.eos_token = tokenizer.convert_tokens_to_ids(\"</s>\")\n",
    "        self.question_prefix_tokens = tokenizer.convert_tokens_to_ids('<q>')\n",
    "\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __getitem__(self, index: int) -> GPTFeaturesType:\n",
    "        example = self.examples[index]\n",
    "\n",
    "        context_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"<c>{example.context}\"))\n",
    "        answer_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"<a>{example.answer}\"))\n",
    "        question_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"{example.question}\"))\n",
    "        \n",
    "        # [SOS] + 문맥:CONTEXT + 정답:ANSWER + 질문:\n",
    "        conditional_tokens_len = 1 + len(context_tokens) + len(answer_tokens) + 1\n",
    "        # QUESTION + [EOS]\n",
    "        post_tokens_len = len(question_tokens) + 1\n",
    "\n",
    "        if conditional_tokens_len + post_tokens_len > self.max_sequence_length:\n",
    "            available_seq_len = (\n",
    "                self.max_sequence_length - conditional_tokens_len - post_tokens_len + len(context_tokens)\n",
    "            )\n",
    "            context_tokens = context_tokens[:available_seq_len]\n",
    "\n",
    "        conditional_tokens = [self.sos_token] + context_tokens + answer_tokens + [self.question_prefix_tokens]\n",
    "        post_tokens = question_tokens + [self.eos_token]\n",
    "        input_ids = conditional_tokens + post_tokens\n",
    "\n",
    "        labels = input_ids if self.is_train else ([-100] * len(conditional_tokens)) + post_tokens\n",
    "        attention_mask = [1.0] * len(input_ids)\n",
    "\n",
    "        assert len(input_ids) <= self.max_sequence_length\n",
    "\n",
    "        return input_ids, attention_mask, labels\n",
    "        #return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(labels)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGDecodingDataset(HistoryQGDataset):\n",
    "    def __getitem__(self, index: int) -> GPTDecodingInputType:\n",
    "        example = self.examples[index]\n",
    "\n",
    "        context_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"<c>{example.context}\"))\n",
    "        answer_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"<a>{example.answer}\"))\n",
    "\n",
    "        # [SOS] + CONTEXT + ANSWER + 정답:\n",
    "        conditional_tokens_len = 1 + len(context_tokens) + len(answer_tokens) + 1\n",
    "        # ANSWER_SEQ + [EOS]\n",
    "        post_tokens_len = MAX_QUESTION_SPACE + 1\n",
    "        if conditional_tokens_len + post_tokens_len > self.max_sequence_length:\n",
    "            available_seq_len = (\n",
    "                self.max_sequence_length - conditional_tokens_len - post_tokens_len + len(context_tokens)\n",
    "            )\n",
    "            context_tokens = context_tokens[:available_seq_len]\n",
    "\n",
    "        input_ids = [self.sos_token] + context_tokens + answer_tokens + [self.question_prefix_tokens]\n",
    "        attention_mask = [1.0] * len(input_ids)\n",
    "        return torch.tensor(input_ids), torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-6-13d665a7688d>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-13d665a7688d>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    for i, batch in enumerate(dataloader):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def QG(model):\n",
    "    model = model.to(device)\n",
    "    model.eval\n",
    "6\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        input_ids, attention_mask = tuple(v.to(device) for v in batch)\n",
    "        origin_seq_len = input_ids.size(-1)\n",
    "\n",
    "        decoded_sequences = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=origin_seq_len + MAX_QUESTION_SPACE,\n",
    "            min_length=origin_seq_len + MIN_QUESTION_SPACE,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            num_beams=5,\n",
    "            repetition_penalty=2.0,\n",
    "        )\n",
    "        \n",
    "        decoded_question_text = ''\n",
    "        for decoded_tokens in decoded_sequences.tolist():\n",
    "\n",
    "            decoded_question_text = tokenizer.decode(decoded_tokens[origin_seq_len:])\n",
    "            decoded_question_text = decoded_question_text.split(\"</s>\")[0].replace(\"<c>\", \"\")\n",
    "            \n",
    "        print(\"- Context: \", examples[i].context)\n",
    "        print(\"- Generated Question: \", decoded_question_text)\n",
    "        print(\"- Answer: \", examples[i].answer)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 생성 모델 비교 \n",
    "## KoGPT2 모델 vs After History Finetuning 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '이순신 장군이'\n",
    "\n",
    "#  Load KoGPT2 model\n",
    "kogpt_model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "input_ids = tokenizer.encode(text)\n",
    "gen_ids = kogpt_model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "kogpt2_generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
    "\n",
    "print(\"KoGPT2\")\n",
    "print(kogpt2_generated)\n",
    "print()\n",
    "\n",
    "# Load After History Finetuning model\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "model.load_state_dict(torch.load('hisotry_finetuning_outputs/gpt2_step_200000.pth', map_location=\"cpu\"))\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file='Tokenizer/tokenizer.json',\n",
    "            bos_token=BOS, eos_token=EOS, unk_token='<unk>', \n",
    "            pad_token=PAD, mask_token=MASK)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
    "\n",
    "print(\"After History Fintuning: \")\n",
    "print(generated)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 퀴즈 생성 비교 \n",
    "## KoGPT2 vs After QA Finetuning Model vs After History&QA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "model.load_state_dict(torch.load('outputs/After_History_Finetuning/gpt2_step_18000.pth'))\n",
    "\n",
    "kogpt_model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "after_QA_model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "after_QA_model.load_state_dict(torch.load('outputs/V2/gpt2_step_18000.pth'))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "examples = []\n",
    "\n",
    "\"\"\"\n",
    "with open('data/as_set.json') as f:\n",
    "    data = json.load(f)\n",
    "count = 0\n",
    "for i in tqdm(data['data']):\n",
    "    example = QAExample(i['doc'], i['answer'])\n",
    "    examples.append(example)\n",
    "    count += 1\n",
    "    if count == 1:\n",
    "        break\n",
    "\"\"\"\n",
    "        \n",
    "example = QAExample(\n",
    "                    '가군은 삼국시대 고구려 장안성 축성의 책임을 맡았던 관리. 가군은 생몰년 미상. 1964년에 평양 중구역 남문동에서 발견된 내성의 성벽돌에 새겨져 있는 명문에 따르면 가군은 성곽 축조시에 소형 관등을 가졌으며, 장안성 내성 중 일정 거리의 성벽 축조를 감독하였음을 알 수 있다.고구려 장안성은 522년에 쌓았고 589년에 장안성으로 천도하였다. 따라서 그의 활동 시기는 대략 6세기 후반 평원왕대로 추정된다. 또 명문의 괘루개절은 가군의 출신지나 직명으로 볼 수 있다.괘루는 고구려 5부의 하나인 계루부의 다른 표기로 볼 수도 있으나 아직 단정하기는 어려우며, 개절은 관직명의 일종으로 추정되고 있다. 이 명문이 새겨진 성벽돌은 현재 인민대학습당 내부를 지나는 성벽의 원위치에 그대로 있다',\n",
    "                    '가군')\n",
    "examples.append(example)\n",
    "dataset = QGDecodingDataset(examples, tokenizer, 512)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, 1)\n",
    "\n",
    "print(\"KoGPT2\")\n",
    "QG(kogpt_model)\n",
    "    \n",
    "print(\"After QA Finetuning Model\")\n",
    "QG(after_QA_model)\n",
    "    \n",
    "print(\"After History&QA Finetuning Model\")\n",
    "QG(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
