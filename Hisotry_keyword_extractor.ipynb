{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/wowns/data/Crawling/data_info.json', \"r\") as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "title = []\n",
    "num = []\n",
    "context = []\n",
    "count = 0\n",
    "for i in json_data['data']:\n",
    "    title.append(i['title'])\n",
    "    num.append(i['count'])\n",
    "    \n",
    "data = {\n",
    "    'title':title,\n",
    "    'num': num,\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "sorted = df.sort_values(by='num' ,ascending=False)\n",
    "sorted.to_csv('sorted_title_by_num.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.func import remove_bracket\n",
    "title_sort = pd.read_csv('/home/hsoh0423/vscode/HistoryQA/sorted_title_by_num.csv')\n",
    "title_rm_list = []\n",
    "title_rm_count = []\n",
    "for i in range(len(title_sort)):\n",
    "    try:\n",
    "        title_rm = remove_bracket(title_sort['title'][i])\n",
    "        title_rm_list.append(title_rm)\n",
    "        title_rm_count.append(title_sort['num'][i])\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "data = {\n",
    "    'title': title_rm_list,\n",
    "    'num': title_rm_count\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('sorted_title_rm.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_sort = pd.read_csv('/home/hsoh0423/vscode/HistoryQA/sorted_title_rm.csv')\n",
    "\n",
    "title2 = []\n",
    "title_count2 = []\n",
    "\n",
    "\n",
    "count = 0\n",
    "for i in tqdm_notebook(range(len(title_sort))):\n",
    "    \n",
    "    if  title_sort['title'][i] in title_list and title_sort['num'][i] > 150:\n",
    "        title2.append(title_sort['title'][i])\n",
    "        title_count2.append(title_sort['num'][i])\n",
    "\n",
    "data = {\n",
    "    'title': title2,\n",
    "    'num': title_count2\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('sorted_title_preQuiz.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "# %%\n",
    "import sys\n",
    "from os import path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "from Embeder import SentenceEmbeder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[transformers.configuration_utils] loading configuration file /home/hsoh0423/kobert/kobert_from_pretrained/config.json\n",
      "[transformers.configuration_utils] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "[transformers.modeling_utils] loading weights file /home/hsoh0423/kobert/kobert_from_pretrained/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "if __package__ is None:\n",
    "    current_path = path.dirname(path.dirname(path.abspath(\n",
    "        '/home/fhdufhdu/vscode/KoreanHistoryProject/keyword_extractor/keyword_extractor_ver_W2V.ipynb')))\n",
    "    sys.path.append(current_path)\n",
    "    from func import save_pickle, load_pickle, get_tokenized_sentences, remove_duplicated_keyword, save_json\n",
    "else:\n",
    "    from func import save_pickle, load_pickle, get_tokenized_sentences, remove_duplicated_keyword, save_json\n",
    "\n",
    "\n",
    "title_list, sent_list = get_tokenized_sentences('.', True)\n",
    "title_doc_list = load_pickle('/home/fhdufhdu/vscode/Project/data/keyword_extractor_data/title_doc_list.pickle')\n",
    "\n",
    "\n",
    "# embedding_model = Word2Vec(sent_list, size=100, window = 5, min_count=1, workers=4, iter=50, sg=1)\n",
    "# save_pickle('w2v_model/w2v.pickle', embedding_model)\n",
    "\n",
    "w2v_model = load_pickle('/home/fhdufhdu/vscode/Project/KoreanHistoryProject/keyword_extractor/w2v_model/w2v.pickle')\n",
    "bert_embeder = SentenceEmbeder(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword(title, words, w2v, bert):\n",
    "    # bert\n",
    "    _, title_vector = bert.encode([title])\n",
    "    _, noun_vectors = bert.encode(words)\n",
    "    title_vector = title_vector[0]\n",
    "\n",
    "\n",
    "    title_embedding_ = [[value.item() for value in title_vector]]\n",
    "    candicates_embedding_ = [[value.item() for value in noun_vector]\n",
    "                            for noun_vector in noun_vectors]\n",
    "    distances = cosine_similarity(title_embedding_, candicates_embedding_)\n",
    "    b_similar_list = [(words[i], sim) for i, sim in enumerate(distances[0])]\n",
    "\n",
    "    # word2vec\n",
    "    w_similar_list = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            w_similar_list.append((word, w2v.wv.similarity(w1=title_list[0], w2=word)))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    final_list = []\n",
    "    for (w_t, w_s), (_, b_s) in zip(w_similar_list, b_similar_list):\n",
    "        final_list.append((w_t, w_s + b_s))\n",
    "\n",
    "    final_list = sorted(final_list, key=lambda sim: sim[1], reverse=True)\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsoh0423/anaconda3/envs/historyQA/lib/python3.7/site-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3495bb8d49104bbb9b36cc6cc62fc5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save as 100\n",
      "save as 200\n",
      "save as 300\n",
      "save as 300\n",
      "save as 400\n",
      "save as 400\n",
      "catch exception\n",
      "save as 500\n",
      "save as 600\n",
      "save as 700\n",
      "save as 700\n",
      "save as 700\n",
      "save as 800\n",
      "save as 900\n",
      "save as 900\n",
      "save as 1000\n",
      "save as 1000\n",
      "save as 1100\n",
      "save as 1200\n",
      "save as 1300\n",
      "save as 1400\n",
      "save as 1500\n",
      "save as 1600\n",
      "save as 1700\n",
      "save as 1800\n",
      "save as 1800\n",
      "save as 1800\n",
      "save as 1900\n",
      "save as 1900\n",
      "save as 2000\n",
      "save as 2100\n",
      "save as 2100\n",
      "save as 2200\n",
      "save as 2300\n",
      "save as 2400\n",
      "save as 2500\n",
      "save as 2500\n",
      "save as 2600\n",
      "save as 2700\n",
      "save as 2700\n",
      "save as 2800\n",
      "save as 2900\n",
      "save as 3000\n",
      "save as 3100\n",
      "save as 3100\n",
      "save as 3200\n",
      "save as 3300\n",
      "save as 3300\n",
      "save as 3400\n",
      "save as 3400\n",
      "save as 3400\n",
      "save as 3400\n",
      "save as 3500\n",
      "save as 3600\n",
      "save as 3700\n",
      "save as 3800\n",
      "save as 3800\n",
      "save as 3900\n",
      "save as 3900\n",
      "save as 4000\n",
      "save as 4100\n",
      "save as 4200\n",
      "save as 4300\n",
      "save as 4400\n",
      "save as 4500\n",
      "save as 4600\n",
      "save as 4700\n",
      "save as 4800\n",
      "save as 4800\n",
      "save as 4900\n",
      "save as 5000\n",
      "save as 5100\n",
      "save as 5200\n",
      "save as 5200\n",
      "save as 5300\n",
      "save as 5400\n",
      "save as 5500\n",
      "save as 5600\n",
      "save as 5600\n",
      "save as 5700\n",
      "save as 5800\n",
      "save as 5900\n",
      "save as 5900\n",
      "save as 6000\n",
      "save as 6000\n",
      "save as 6100\n",
      "save as 6100\n",
      "save as 6100\n",
      "save as 6200\n",
      "save as 6300\n",
      "save as 6300\n",
      "save as 6400\n",
      "save as 6500\n",
      "save as 6500\n",
      "save as 6500\n",
      "save as 6600\n",
      "save as 6700\n",
      "save as 6800\n",
      "save as 6900\n",
      "save as 7000\n",
      "save as 7100\n",
      "save as 7200\n",
      "save as 7300\n",
      "save as 7400\n",
      "save as 7500\n",
      "save as 7600\n",
      "save as 7700\n"
     ]
    }
   ],
   "source": [
    "quiz_title_list = pd.read_csv('/home/hsoh0423/vscode/HistoryQA/test_title.csv')\n",
    "quiz_title_list2 = []\n",
    "for i in quiz_title_list['title']:\n",
    "    quiz_title_list2.append(i)\n",
    "\n",
    "qas_dict = {}\n",
    "qas_dict['data'] = []\n",
    "count = 0\n",
    "for i in tqdm_notebook(quiz_title_list2):\n",
    "    try:\n",
    "        doc_idx = 0\n",
    "        sent_idx = 0\n",
    "        count += 1\n",
    "        while sent_idx < len(title_doc_list):\n",
    "            title = title_list[sent_idx]\n",
    "            title_with_doc, doc = title_doc_list[sent_idx]\n",
    "            sent = sent_list[sent_idx]\n",
    "\n",
    "            if i == title:\n",
    "                qas_set = {}\n",
    "                qas_set['title'] = title\n",
    "                qas_set['doc'] = doc\n",
    "                qas_set['answer'] = title\n",
    "                qas_dict['data'].append(qas_set)\n",
    "                if title == title_with_doc:\n",
    "\n",
    "                        keywords = get_keyword(title, remove_duplicated_keyword(sent), w2v_model, bert_embeder)[:5]\n",
    "                        for keyword, _ in keywords:\n",
    "                            qas_set = {}\n",
    "                            qas_set['title'] = title\n",
    "                            qas_set['doc'] = doc\n",
    "                            qas_set['answer'] = keyword\n",
    "                            qas_dict['data'].append(qas_set)\n",
    "\n",
    "                if count % 100 == 0 :\n",
    "                    print(\"save as\", count)\n",
    "                    save_json('as_set_V2.json', qas_dict)\n",
    "            sent_idx += 1  \n",
    "    except:\n",
    "        print('catch exception')\n",
    "        continue\n",
    "        \n",
    "save_json('as_set_V2.json', qas_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/hsoh0423/vscode/HistoryQA/as_set_test.json', \"r\") as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "title_list_count = []\n",
    "for i in json_data['data']:\n",
    "        title_list_count.append(i['title'])\n",
    "        \n",
    "title_list_count2 = list(set(title_list_count))\n",
    "print(len(title_list_count2))\n",
    "for i in quiz_title_list2[:471]:\n",
    "        if i not in title_list_count2:\n",
    "                print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json('as_set_test.json', qas_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "qas_dict = {}\n",
    "qas_dict['data'] = []\n",
    "doc_idx = 0\n",
    "sent_idx = 0\n",
    "while sent_idx < 10 and doc_idx < len(title_doc_list):\n",
    "    title = title_list[sent_idx]\n",
    "    title_with_doc, doc = title_doc_list[doc_idx]\n",
    "    sent = sent_list[sent_idx]\n",
    "    if title != title_with_doc:\n",
    "        doc_idx += 1\n",
    "        continue\n",
    "    print(title, title_with_doc)\n",
    "    keywords = get_keyword(title, remove_duplicated_keyword(\n",
    "        sent), w2v_model, bert_embeder)[:5]\n",
    "    for keyword, _ in keywords:\n",
    "        qas_set = {}\n",
    "        qas_set['doc'] = doc\n",
    "        qas_set['answer'] = keyword\n",
    "        qas_dict['data'].append(qas_set)\n",
    "    qas_set = {}\n",
    "    qas_set['doc'] = doc\n",
    "    qas_set['answer'] = [title]\n",
    "    qas_dict['data'].append(qas_set)\n",
    "    sent_idx += 1\n",
    "    print('progress : {}/{}'.format(sent_idx, 5000))\n",
    "\n",
    "save_json('as_set_test.json', qas_dict)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "acf6e4abe0c500b723931f115c3e38565af0a1699b33a74e0337a5d37d48ad31"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('historyQA': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
