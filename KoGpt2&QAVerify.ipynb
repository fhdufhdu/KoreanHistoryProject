{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "# Initializing a GPT2 configuration\n",
    "configuration = GPT2Config()\n",
    "\n",
    "# Initializing a model from the configuration\n",
    "model = GPT2Model(config=GPT2Config.from_json_file('config.json'))\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "#from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm_notebook\n",
    "from transformers import GPT2LMHeadModel, get_linear_schedule_with_warmup\n",
    "from korquad_qg.config import QGConfig\n",
    "from korquad_qg.utils import TqdmLoggingHandler\n",
    "from typing import List, NamedTuple, Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 코쿼드 데이터셋을 이용한 GPT2 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "with open('KorQuAD_v1.0_train.json', 'r') as f:\n",
    "\n",
    "    json_data = json.load(f)\n",
    "print(len(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('KorQuAD_v1.0_train.json', 'r') as f:\n",
    "\n",
    "    json_data = json.load(f)\n",
    "\n",
    "context_list = []\n",
    "answers_list = []\n",
    "question_list = []\n",
    "\n",
    "for data in json_data['data']:\n",
    "    for sub_data in data['paragraphs']:\n",
    "        context = sub_data['context']\n",
    "        for qa in sub_data['qas']:\n",
    "            context_list.append(context)\n",
    "            answers_list.append(qa['answers'][0]['text'])\n",
    "            question_list.append(qa['question'])\n",
    "            if len(qa['answers']) > 1:\n",
    "                print(qa['answers'])\n",
    "                \n",
    "data = {\n",
    "        'context':context_list,\n",
    "        'answers': answers_list,\n",
    "        'question':question_list\n",
    "    }\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('KorQuad_dev_V1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토크나이저 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_TKN = '<q>'\n",
    "A_TKN = '<a>'\n",
    "BOS = '<s>'\n",
    "EOS = '</s>'\n",
    "MASK = '<mask>'\n",
    "C_TKN = '<c>'\n",
    "PAD = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_f = open(\"after_mecab.txt\", 'r')\n",
    "all_dict = []\n",
    "max_len = 0\n",
    "count = 0\n",
    "for line in tqdm_notebook(txt_f.readlines()):\n",
    "    if '\\n' in line:\n",
    "        line = line.replace('\\n', '')\n",
    "    if '##' in line:\n",
    "        line = line.replace('##', '')\n",
    "    all_dict.append(line)\n",
    "    count += 1\n",
    "print(count)\n",
    "\n",
    "\n",
    "f = open(\"data_save_test.txt\", 'w')\n",
    "for i in all_dict:\n",
    "    f.write(i+'\\n')\n",
    "f.close()\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "# Then train it!\n",
    "\n",
    "tokenizer.train_from_iterator([all_dict], vocab_size=3000000, min_frequency=1, limit_alphabet=100000,\n",
    "            special_tokens=[PAD, BOS, EOS, MASK,'<unk>', '<q>', '<a>', '<c>'])\n",
    "\n",
    "tokenizer.save(\"./history_Korquad_tokenizer.json\",pretty=True)\n",
    "tokenizer.save_model(directory='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_f = open(\"sentences.txt\", 'r')\n",
    "max_len = 512\n",
    "count = 0\n",
    "tok = PreTrainedTokenizerFast(tokenizer_file='Tokenizer/History/history_tokenizer.json')\n",
    "for line in tqdm_notebook(txt_f.readlines()):\n",
    "    if '\\n' in line:\n",
    "        line = line.replace('\\n', '')\n",
    "    if '##' in line:\n",
    "        line = line.replace('##', '')\n",
    "    if len(tok.tokenize(line)) + 7 > max_len:\n",
    "        max_len = len(line)\n",
    "        count += 1\n",
    "print(max_len, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = PreTrainedTokenizerFast(tokenizer_file='Tokenizer/ History_Korquad_Tokenizer/history_Korquad_tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.convert_tokens_to_ids(\"<s>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_TKN = '<q>'\n",
    "A_TKN = '<a>'\n",
    "BOS = '</s>'\n",
    "EOS = '</s>'\n",
    "MASK = '<mask>'\n",
    "C_TKN = '<c>'\n",
    "PAD = '<pad>'\n",
    "TOKENIZER = PreTrainedTokenizerFast(tokenizer_file='Tokenizer/tokenizer.json',\n",
    "            bos_token=BOS, eos_token=EOS, unk_token='<unk>', \n",
    "            pad_token=PAD, mask_token=MASK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 셋 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPTDecodingInputType = Tuple[torch.Tensor, torch.Tensor]\n",
    "GPTInputsType = Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "GPTFeaturesType = Tuple[List[int], List[float], List[int]]\n",
    "\n",
    "class QAExample(NamedTuple):\n",
    "    context: str\n",
    "    answer: str\n",
    "    question: Optional[str] = None\n",
    "\n",
    "def load_korquad_dataset(dataset_path: str) -> List[QAExample]:\n",
    "    korquad = [pd.read_csv(dataset_path)]\n",
    "    max_len = 512\n",
    "    examples = []\n",
    "    for document in korquad:\n",
    "        for i in tqdm_notebook(range(len(document))):\n",
    "            if len(TOKENIZER.tokenize(document[\"context\"][i])) + 10 <= max_len:\n",
    "                example = QAExample(document[\"context\"][i], document[\"answers\"][i], document[\"question\"][i])\n",
    "                examples.append(example)\n",
    "        \n",
    "    return examples\n",
    "    \n",
    "def dynamic_padding_collate_fn(features: List[GPTFeaturesType]) -> GPTInputsType:\n",
    "    max_seq_len = max([len(feature[0]) for feature in features])\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "\n",
    "    for feature in features:\n",
    "        padded_input_ids = feature[0] + [0] * (max_seq_len - len(feature[0]))\n",
    "        padded_attention_mask = feature[1] + [0.0] * (max_seq_len - len(feature[1]))\n",
    "        padded_labels = feature[2] + [-100] * (max_seq_len - len(feature[2]))\n",
    "\n",
    "        input_ids.append(padded_input_ids)\n",
    "        attention_mask.append(padded_attention_mask)\n",
    "        labels.append(padded_labels)\n",
    "\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HistoryQGDataset(Dataset):\n",
    "    def __init__(self, examples: List[QAExample], max_len=32):\n",
    "        self.data = examples\n",
    "        self.first = True\n",
    "        self.c_token = C_TKN\n",
    "        self.a_token = A_TKN\n",
    "        self.q_token = Q_TKN\n",
    "        self.bos = BOS\n",
    "        self.eos = EOS\n",
    "        self.mask = MASK\n",
    "        self.pad = PAD\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = TOKENIZER \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx)-> GPTFeaturesType:\n",
    "        turn = self.data[idx]\n",
    "        c = turn.context\n",
    "        a = turn.answer\n",
    "        q = turn.question\n",
    "\n",
    "        ca_toked = self.tokenizer.tokenize(self.c_token + c + self.bos+ self.a_token + a + self.bos)\n",
    "        ca_len = len(ca_toked)\n",
    "        q_toked = self.tokenizer.tokenize(self.q_token + q + self.eos)\n",
    "        q_len = len(q_toked)\n",
    "\n",
    "        if ca_len + q_len > self.max_len:\n",
    "            q_len = self.max_len - ca_len\n",
    "            if q_len <= 0:\n",
    "                ca_toked = ca_toked[-(int(self.max_len/2)):]\n",
    "                ca_len = len(ca_toked)\n",
    "                q_len = self.max_len - ca_len\n",
    "                assert q_len > 0\n",
    "            q_toked = q_toked[:q_len]\n",
    "            q_len = len(q_toked)\n",
    "            assert q_len == len(q_toked), f'{q_len} ==? {len(q_toked)}'\n",
    "\n",
    "\n",
    "        labels = [\n",
    "            self.mask,\n",
    "        ] * ca_len + q_toked[1:]\n",
    "\n",
    "        if self.first:\n",
    "            logging.info(\"contexts : {}\".format(c))\n",
    "            logging.info(\"toked ctx: {}\".format(ca_toked))\n",
    "            logging.info(\"response : {}\".format(q))\n",
    "            logging.info(\"toked response : {}\".format(q_toked))\n",
    "            logging.info('labels {}'.format(labels))\n",
    "            self.first = False\n",
    "\n",
    "        mask = [0] * ca_len + [1] * q_len + [0] * (self.max_len - ca_len - q_len)\n",
    "        self.max_len\n",
    "        \n",
    "        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)\n",
    "        while len(labels_ids) < self.max_len:\n",
    "            labels_ids += [self.tokenizer.pad_token_id]\n",
    "\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(ca_toked + q_toked)\n",
    "        while len(token_ids) < self.max_len:\n",
    "            token_ids += [self.tokenizer.pad_token_id]\n",
    "\n",
    "        return token_ids, np.array(mask), labels_ids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryQGDataset2(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        examples: List[QAExample],\n",
    "        tokenizer: SentencePieceBPETokenizer,\n",
    "        max_sequence_length: int,\n",
    "        is_train: bool = True,\n",
    "    ) -> None:\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "        self.sos_token = tokenizer.convert_tokens_to_ids(\"<c>\")\n",
    "        self.eos_token = tokenizer.convert_tokens_to_ids(\"</s>\")\n",
    "        self.question_prefix_tokens = tokenizer.convert_tokens_to_ids('<q>')\n",
    "\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __getitem__(self, index: int) -> GPTFeaturesType:\n",
    "        example = self.examples[index]\n",
    "\n",
    "        context_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"<c>{example.context}\"))\n",
    "        answer_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"<a>{example.answer}\"))\n",
    "        question_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"{example.question}\"))\n",
    "        \n",
    "        # [SOS] + 문맥:CONTEXT + 정답:ANSWER + 질문:\n",
    "        conditional_tokens_len = 1 + len(context_tokens) + len(answer_tokens) + 1\n",
    "        # QUESTION + [EOS]\n",
    "        post_tokens_len = len(question_tokens) + 1\n",
    "\n",
    "        if conditional_tokens_len + post_tokens_len > self.max_sequence_length:\n",
    "            available_seq_len = (\n",
    "                self.max_sequence_length - conditional_tokens_len - post_tokens_len + len(context_tokens)\n",
    "            )\n",
    "            context_tokens = context_tokens[:available_seq_len]\n",
    "\n",
    "        conditional_tokens = [self.sos_token] + context_tokens + answer_tokens + [self.question_prefix_tokens]\n",
    "        post_tokens = question_tokens + [self.eos_token]\n",
    "        input_ids = conditional_tokens + post_tokens\n",
    "\n",
    "        labels = input_ids if self.is_train else ([-100] * len(conditional_tokens)) + post_tokens\n",
    "        attention_mask = [1.0] * len(input_ids)\n",
    "\n",
    "        assert len(input_ids) <= self.max_sequence_length\n",
    "\n",
    "        return input_ids, attention_mask, labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_logger(output_dir: str):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"[%(asctime)s] %(message)s\")\n",
    "\n",
    "    file_handler = logging.FileHandler(os.path.join(output_dir, \"train.log\"))\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    handler = TqdmLoggingHandler()\n",
    "    handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\"))\n",
    "    logger.addHandler(handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--train-dataset\", type=str, help=\"학습 데이터 경로dd\")\n",
    "parser.add_argument(\"--dev-dataset\", type=str, help=\"평가 데이터 경로\")\n",
    "\n",
    "parser.add_argument(\"--epochs\", type=int, help=\"학습 전체를 반복할 횟수\")\n",
    "parser.add_argument(\"--lr\", type=float, help=\"learning rate\")\n",
    "\n",
    "parser.add_argument(\"--train-batch-size\", type=int, help=\"학습에 사용할 배치 크기\")\n",
    "parser.add_argument(\"--eval-batch-size\", type=int, help=\"평가에 사용할 배치 크기\")\n",
    "parser.add_argument(\"--validation-interval\", type=int, help=\"dev 셋에 대해서 validation 을 수행할 steps\")\n",
    "parser.add_argument(\"--save-interval\", type=int, help=\"모델을 저장할 steps\")\n",
    "\n",
    "parser.add_argument(\"--output-dir\", type=str, default=\"artifacts/\", help=\"모델과 학습 로그를 저장할 경로\")\n",
    "config = QGConfig()\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file='Tokenizer/tokenizer.json',\n",
    "            bos_token=BOS, eos_token=EOS, unk_token='<unk>', \n",
    "            pad_token=PAD, mask_token=MASK)\n",
    "\n",
    "logger = _create_logger(output_dir=config.output_dir)\n",
    "logger.info(\"============================\")\n",
    "for key, value in config._asdict().items():\n",
    "    logger.info(f\"{key:30}:{value}\")\n",
    "logger.info(\"============================\")\n",
    "torch.manual_seed(config.random_seed)\n",
    "logger.info(\"loading train dataset\")\n",
    "\n",
    "\n",
    "\n",
    "train_examples = load_korquad_dataset(config.train_dataset)\n",
    "train_dataset = HistoryQGDataset2(train_examples, tokenizer, config.max_sequence_length)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, 2, shuffle=True, collate_fn=dynamic_padding_collate_fn)\n",
    "\n",
    "# model 생성\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "model.resize_token_embeddings()\n",
    "print(model.transformer.wte.weight.shape[0], len(TOKENIZER.vocab))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=config.lr)\n",
    "total_steps = len(train_dataloader) * config.epochs\n",
    "warmup_steps = int(total_steps * config.warmup_ratio)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "loss_list_between_log_interval = []\n",
    "for epoch_id in range(config.epochs):\n",
    "   for step_index, batch_data in tqdm_notebook(\n",
    "            enumerate(train_dataloader), f\"[TRAIN] EP:{epoch_id}\", total=len(train_dataloader)\n",
    "    ):\n",
    "        global_step = len(train_dataloader) * epoch_id + step_index + 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        token_ids, attention_mask, labels = tuple(value.to(device) for value in batch_data)\n",
    "        print(token_ids.shape, attention_mask.shape, labels.shape)\n",
    "        model_outputs = model.forward(token_ids, attention_mask=attention_mask, labels=labels, return_dict=True)\n",
    "        model_outputs.loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # for logging\n",
    "        loss_list_between_log_interval.append(model_outputs.loss.item())\n",
    "\n",
    "        if global_step % config.train_log_interval == 0:\n",
    "            mean_loss = np.mean(loss_list_between_log_interval)\n",
    "            loss_list_between_log_interval.clear()\n",
    "\n",
    "        if global_step % config.save_interval == 0:\n",
    "            state_dict = model.state_dict()\n",
    "            model_path = os.path.join(config.output_dir, f\"gpt2_step_{global_step}.pth\")\n",
    "            torch.save(state_dict, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332430\n"
     ]
    }
   ],
   "source": [
    "with open('/home/wowns/data/KoreanHistoryProject/QA/QA_Original/as_generate_set.json', 'r') as f:\n",
    "\n",
    "    json_data = json.load(f)\n",
    "with open('/home/wowns/data/KoreanHistoryProject/QA/QA_Original/as_generate_set2.json', 'r') as f:\n",
    "\n",
    "    json_data2 = json.load(f)\n",
    "\n",
    "all_list = []\n",
    "count = 0\n",
    "\n",
    "for i in json_data[\"data\"]:\n",
    "    all_list.append(i)\n",
    "    count +=1\n",
    "\n",
    "for i in json_data2[\"data\"]:\n",
    "    all_list.append(i)\n",
    "    count +=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...initailizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsoh0423/anaconda3/envs/historyQA/lib/python3.7/site-packages/ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da6041a01d443fea0bce70655a25275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/332430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0 correct_count:  1 except_count 0\n",
      "step:  1000 correct_count:  151 except_count 15\n",
      "step:  2000 correct_count:  299 except_count 30\n",
      "step:  3000 correct_count:  428 except_count 45\n",
      "step:  4000 correct_count:  570 except_count 60\n",
      "step:  5000 correct_count:  708 except_count 105\n",
      "step:  6000 correct_count:  819 except_count 135\n",
      "step:  7000 correct_count:  937 except_count 194\n",
      "step:  8000 correct_count:  1088 except_count 269\n",
      "step:  9000 correct_count:  1244 except_count 314\n",
      "step:  10000 correct_count:  1374 except_count 341\n",
      "step:  11000 correct_count:  1498 except_count 371\n",
      "step:  12000 correct_count:  1641 except_count 371\n",
      "step:  13000 correct_count:  1794 except_count 386\n",
      "step:  14000 correct_count:  1939 except_count 401\n",
      "step:  15000 correct_count:  2073 except_count 416\n",
      "step:  16000 correct_count:  2219 except_count 416\n",
      "step:  17000 correct_count:  2339 except_count 431\n",
      "step:  18000 correct_count:  2507 except_count 431\n",
      "step:  19000 correct_count:  2616 except_count 446\n",
      "step:  20000 correct_count:  2768 except_count 476\n",
      "step:  21000 correct_count:  2941 except_count 491\n",
      "step:  22000 correct_count:  3067 except_count 536\n",
      "step:  23000 correct_count:  3223 except_count 566\n",
      "step:  24000 correct_count:  3373 except_count 641\n",
      "step:  25000 correct_count:  3554 except_count 656\n",
      "step:  26000 correct_count:  3673 except_count 746\n",
      "step:  27000 correct_count:  3840 except_count 761\n",
      "step:  28000 correct_count:  4013 except_count 762\n",
      "step:  29000 correct_count:  4177 except_count 777\n",
      "step:  30000 correct_count:  4330 except_count 807\n",
      "step:  31000 correct_count:  4476 except_count 822\n",
      "step:  32000 correct_count:  4623 except_count 856\n",
      "step:  33000 correct_count:  4770 except_count 871\n",
      "step:  34000 correct_count:  4896 except_count 871\n",
      "step:  35000 correct_count:  5020 except_count 916\n",
      "step:  36000 correct_count:  5159 except_count 916\n",
      "step:  37000 correct_count:  5291 except_count 916\n",
      "step:  38000 correct_count:  5446 except_count 916\n",
      "step:  39000 correct_count:  5609 except_count 916\n",
      "step:  40000 correct_count:  5761 except_count 957\n",
      "step:  41000 correct_count:  5905 except_count 972\n",
      "step:  42000 correct_count:  6073 except_count 987\n",
      "step:  43000 correct_count:  6223 except_count 1092\n",
      "step:  44000 correct_count:  6368 except_count 1092\n",
      "step:  45000 correct_count:  6501 except_count 1137\n",
      "step:  46000 correct_count:  6628 except_count 1182\n",
      "step:  47000 correct_count:  6766 except_count 1230\n",
      "step:  48000 correct_count:  6898 except_count 1246\n",
      "step:  49000 correct_count:  7050 except_count 1320\n",
      "step:  50000 correct_count:  7214 except_count 1335\n",
      "step:  51000 correct_count:  7354 except_count 1335\n",
      "step:  52000 correct_count:  7501 except_count 1350\n",
      "step:  53000 correct_count:  7641 except_count 1380\n",
      "step:  54000 correct_count:  7791 except_count 1455\n",
      "step:  55000 correct_count:  7960 except_count 1473\n",
      "step:  56000 correct_count:  8113 except_count 1581\n",
      "step:  57000 correct_count:  8290 except_count 1611\n",
      "step:  58000 correct_count:  8430 except_count 1624\n",
      "step:  59000 correct_count:  8546 except_count 1624\n",
      "step:  60000 correct_count:  8703 except_count 1624\n",
      "step:  61000 correct_count:  8842 except_count 1624\n",
      "step:  62000 correct_count:  8991 except_count 1669\n",
      "step:  63000 correct_count:  9141 except_count 1699\n",
      "step:  64000 correct_count:  9290 except_count 1714\n",
      "step:  65000 correct_count:  9402 except_count 1714\n",
      "step:  66000 correct_count:  9508 except_count 1729\n",
      "step:  67000 correct_count:  9658 except_count 1761\n",
      "step:  68000 correct_count:  9775 except_count 1776\n",
      "step:  69000 correct_count:  9907 except_count 1806\n",
      "step:  70000 correct_count:  10016 except_count 1821\n",
      "step:  71000 correct_count:  10158 except_count 1866\n",
      "step:  72000 correct_count:  10303 except_count 1882\n",
      "step:  73000 correct_count:  10417 except_count 1911\n",
      "step:  74000 correct_count:  10566 except_count 1911\n",
      "step:  75000 correct_count:  10724 except_count 1926\n",
      "step:  76000 correct_count:  10845 except_count 1941\n",
      "step:  77000 correct_count:  10857 except_count 1971\n",
      "step:  78000 correct_count:  10872 except_count 2016\n",
      "step:  79000 correct_count:  10876 except_count 2031\n",
      "step:  80000 correct_count:  10887 except_count 2069\n",
      "step:  81000 correct_count:  10892 except_count 2099\n",
      "step:  82000 correct_count:  10915 except_count 2129\n",
      "step:  83000 correct_count:  10921 except_count 2159\n",
      "step:  84000 correct_count:  10928 except_count 2174\n",
      "step:  85000 correct_count:  10939 except_count 2219\n",
      "step:  86000 correct_count:  10949 except_count 2234\n",
      "step:  87000 correct_count:  10957 except_count 2279\n",
      "step:  88000 correct_count:  10981 except_count 2294\n",
      "step:  89000 correct_count:  10981 except_count 2354\n",
      "step:  90000 correct_count:  10987 except_count 2384\n",
      "step:  91000 correct_count:  10989 except_count 2431\n",
      "step:  92000 correct_count:  10990 except_count 2461\n",
      "step:  93000 correct_count:  10998 except_count 2506\n",
      "step:  94000 correct_count:  11007 except_count 2551\n",
      "step:  95000 correct_count:  11011 except_count 2566\n",
      "step:  96000 correct_count:  11015 except_count 2596\n",
      "step:  97000 correct_count:  11020 except_count 2611\n",
      "step:  98000 correct_count:  11030 except_count 2626\n",
      "step:  99000 correct_count:  11039 except_count 2686\n",
      "step:  100000 correct_count:  11042 except_count 2716\n",
      "step:  101000 correct_count:  11049 except_count 2761\n",
      "step:  102000 correct_count:  11057 except_count 2821\n",
      "step:  103000 correct_count:  11069 except_count 2865\n",
      "step:  104000 correct_count:  11077 except_count 2888\n",
      "step:  105000 correct_count:  11083 except_count 2918\n",
      "step:  106000 correct_count:  11083 except_count 2955\n",
      "step:  107000 correct_count:  11087 except_count 2970\n",
      "step:  108000 correct_count:  11092 except_count 2970\n",
      "step:  109000 correct_count:  11102 except_count 2970\n",
      "step:  110000 correct_count:  11111 except_count 3000\n",
      "step:  111000 correct_count:  11116 except_count 3000\n",
      "step:  112000 correct_count:  11123 except_count 3030\n",
      "step:  113000 correct_count:  11130 except_count 3060\n",
      "step:  114000 correct_count:  11146 except_count 3090\n",
      "step:  115000 correct_count:  11165 except_count 3105\n",
      "step:  116000 correct_count:  11169 except_count 3105\n",
      "step:  117000 correct_count:  11174 except_count 3195\n",
      "step:  118000 correct_count:  11175 except_count 3210\n",
      "step:  119000 correct_count:  11182 except_count 3210\n",
      "step:  120000 correct_count:  11193 except_count 3225\n",
      "step:  121000 correct_count:  11196 except_count 3255\n",
      "step:  122000 correct_count:  11203 except_count 3285\n",
      "step:  123000 correct_count:  11205 except_count 3345\n",
      "step:  124000 correct_count:  11211 except_count 3345\n",
      "step:  125000 correct_count:  11224 except_count 3360\n",
      "step:  126000 correct_count:  11235 except_count 3390\n",
      "step:  127000 correct_count:  11237 except_count 3435\n",
      "step:  128000 correct_count:  11246 except_count 3480\n",
      "step:  129000 correct_count:  11255 except_count 3509\n",
      "step:  130000 correct_count:  11263 except_count 3541\n",
      "step:  131000 correct_count:  11273 except_count 3541\n",
      "step:  132000 correct_count:  11279 except_count 3586\n",
      "step:  133000 correct_count:  11290 except_count 3616\n",
      "step:  134000 correct_count:  11294 except_count 3631\n",
      "step:  135000 correct_count:  11304 except_count 3646\n",
      "step:  136000 correct_count:  11319 except_count 3661\n",
      "step:  137000 correct_count:  11320 except_count 3661\n",
      "step:  138000 correct_count:  11320 except_count 3691\n",
      "step:  139000 correct_count:  11322 except_count 3721\n",
      "step:  140000 correct_count:  11329 except_count 3766\n",
      "step:  141000 correct_count:  11338 except_count 3766\n",
      "step:  142000 correct_count:  11345 except_count 3766\n",
      "step:  143000 correct_count:  11370 except_count 3766\n",
      "step:  144000 correct_count:  11395 except_count 3766\n",
      "step:  145000 correct_count:  11404 except_count 3781\n",
      "step:  146000 correct_count:  11407 except_count 3781\n",
      "step:  147000 correct_count:  11429 except_count 3781\n",
      "step:  148000 correct_count:  11445 except_count 3811\n",
      "step:  149000 correct_count:  11453 except_count 3811\n",
      "step:  150000 correct_count:  11478 except_count 3811\n",
      "step:  151000 correct_count:  11494 except_count 3841\n",
      "step:  152000 correct_count:  11502 except_count 3856\n",
      "step:  153000 correct_count:  11515 except_count 3871\n",
      "step:  154000 correct_count:  11529 except_count 3886\n",
      "step:  155000 correct_count:  11550 except_count 3916\n",
      "step:  156000 correct_count:  11568 except_count 3937\n",
      "step:  157000 correct_count:  11579 except_count 3967\n",
      "step:  158000 correct_count:  11599 except_count 3997\n",
      "step:  159000 correct_count:  11611 except_count 4012\n",
      "step:  160000 correct_count:  11630 except_count 4098\n",
      "step:  161000 correct_count:  11659 except_count 4102\n",
      "step:  162000 correct_count:  11696 except_count 4102\n",
      "step:  163000 correct_count:  11711 except_count 4132\n",
      "step:  164000 correct_count:  11739 except_count 4132\n",
      "step:  165000 correct_count:  11760 except_count 4132\n",
      "step:  166000 correct_count:  11772 except_count 4192\n",
      "step:  167000 correct_count:  11788 except_count 4192\n",
      "step:  168000 correct_count:  11794 except_count 4237\n",
      "step:  169000 correct_count:  11801 except_count 4252\n",
      "step:  170000 correct_count:  11821 except_count 4267\n",
      "step:  171000 correct_count:  11845 except_count 4342\n",
      "step:  172000 correct_count:  11881 except_count 4372\n",
      "step:  173000 correct_count:  11911 except_count 4372\n",
      "step:  174000 correct_count:  11926 except_count 4417\n",
      "step:  175000 correct_count:  11946 except_count 4417\n",
      "step:  176000 correct_count:  11960 except_count 4417\n",
      "step:  177000 correct_count:  11963 except_count 4447\n",
      "step:  178000 correct_count:  11984 except_count 4477\n",
      "step:  179000 correct_count:  11996 except_count 4477\n",
      "step:  180000 correct_count:  12009 except_count 4540\n",
      "step:  181000 correct_count:  12038 except_count 4540\n",
      "step:  182000 correct_count:  12069 except_count 4555\n",
      "step:  183000 correct_count:  12082 except_count 4566\n",
      "step:  184000 correct_count:  12097 except_count 4611\n",
      "step:  185000 correct_count:  12102 except_count 4611\n",
      "step:  186000 correct_count:  12117 except_count 4642\n",
      "step:  187000 correct_count:  12130 except_count 4642\n",
      "step:  188000 correct_count:  12152 except_count 4657\n",
      "step:  189000 correct_count:  12170 except_count 4657\n",
      "step:  190000 correct_count:  12194 except_count 4672\n",
      "step:  191000 correct_count:  12221 except_count 4748\n",
      "step:  192000 correct_count:  12243 except_count 4748\n",
      "step:  193000 correct_count:  12270 except_count 4748\n",
      "step:  194000 correct_count:  12294 except_count 4778\n",
      "step:  195000 correct_count:  12318 except_count 4793\n",
      "step:  196000 correct_count:  12332 except_count 4793\n",
      "step:  197000 correct_count:  12345 except_count 4808\n",
      "step:  198000 correct_count:  12377 except_count 4838\n",
      "step:  199000 correct_count:  12393 except_count 4868\n",
      "step:  200000 correct_count:  12416 except_count 4870\n",
      "step:  201000 correct_count:  12437 except_count 4870\n",
      "step:  202000 correct_count:  12439 except_count 4885\n",
      "step:  203000 correct_count:  12442 except_count 4900\n",
      "step:  204000 correct_count:  12448 except_count 4933\n",
      "step:  205000 correct_count:  12522 except_count 4963\n",
      "step:  206000 correct_count:  12624 except_count 4978\n",
      "step:  207000 correct_count:  12757 except_count 4993\n",
      "step:  208000 correct_count:  12882 except_count 5034\n",
      "step:  209000 correct_count:  13026 except_count 5068\n",
      "step:  210000 correct_count:  13180 except_count 5148\n",
      "step:  211000 correct_count:  13340 except_count 5172\n",
      "step:  212000 correct_count:  13444 except_count 5187\n",
      "step:  213000 correct_count:  13577 except_count 5262\n",
      "step:  214000 correct_count:  13703 except_count 5305\n",
      "step:  215000 correct_count:  13844 except_count 5350\n",
      "step:  216000 correct_count:  13986 except_count 5365\n",
      "step:  217000 correct_count:  14107 except_count 5395\n",
      "step:  218000 correct_count:  14262 except_count 5395\n",
      "step:  219000 correct_count:  14420 except_count 5410\n",
      "step:  220000 correct_count:  14536 except_count 5470\n",
      "step:  221000 correct_count:  14685 except_count 5515\n",
      "step:  222000 correct_count:  14801 except_count 5612\n",
      "step:  223000 correct_count:  14935 except_count 5627\n",
      "step:  224000 correct_count:  15076 except_count 5657\n",
      "step:  225000 correct_count:  15233 except_count 5657\n",
      "step:  226000 correct_count:  15341 except_count 5672\n",
      "step:  227000 correct_count:  15430 except_count 5717\n",
      "step:  228000 correct_count:  15548 except_count 5747\n",
      "step:  229000 correct_count:  15626 except_count 5762\n",
      "step:  230000 correct_count:  15715 except_count 5777\n",
      "step:  231000 correct_count:  15809 except_count 5837\n",
      "step:  232000 correct_count:  15896 except_count 5837\n",
      "step:  233000 correct_count:  16003 except_count 5837\n",
      "step:  234000 correct_count:  16113 except_count 5857\n",
      "step:  235000 correct_count:  16238 except_count 5875\n",
      "step:  236000 correct_count:  16377 except_count 5950\n",
      "step:  237000 correct_count:  16526 except_count 5965\n",
      "step:  238000 correct_count:  16641 except_count 6040\n",
      "step:  239000 correct_count:  16771 except_count 6070\n",
      "step:  240000 correct_count:  16927 except_count 6115\n",
      "step:  241000 correct_count:  17032 except_count 6160\n",
      "step:  242000 correct_count:  17167 except_count 6210\n",
      "step:  243000 correct_count:  17309 except_count 6252\n",
      "step:  244000 correct_count:  17462 except_count 6286\n",
      "step:  245000 correct_count:  17612 except_count 6329\n",
      "step:  246000 correct_count:  17770 except_count 6339\n",
      "step:  247000 correct_count:  17919 except_count 6368\n",
      "step:  248000 correct_count:  18063 except_count 6368\n",
      "step:  249000 correct_count:  18204 except_count 6384\n",
      "step:  250000 correct_count:  18307 except_count 6473\n",
      "step:  251000 correct_count:  18447 except_count 6514\n",
      "step:  252000 correct_count:  18587 except_count 6514\n",
      "step:  253000 correct_count:  18738 except_count 6541\n",
      "step:  254000 correct_count:  18860 except_count 6541\n",
      "step:  255000 correct_count:  19008 except_count 6541\n",
      "step:  256000 correct_count:  19108 except_count 6541\n",
      "step:  257000 correct_count:  19216 except_count 6586\n",
      "step:  258000 correct_count:  19310 except_count 6601\n",
      "step:  259000 correct_count:  19414 except_count 6646\n",
      "step:  260000 correct_count:  19515 except_count 6661\n",
      "step:  261000 correct_count:  19635 except_count 6676\n",
      "step:  262000 correct_count:  19770 except_count 6721\n",
      "step:  263000 correct_count:  19893 except_count 6779\n",
      "step:  264000 correct_count:  20007 except_count 6806\n",
      "step:  265000 correct_count:  20138 except_count 6847\n",
      "step:  266000 correct_count:  20314 except_count 6881\n",
      "step:  267000 correct_count:  20458 except_count 6896\n",
      "step:  268000 correct_count:  20602 except_count 6896\n",
      "step:  269000 correct_count:  20746 except_count 6911\n",
      "step:  270000 correct_count:  20893 except_count 6941\n",
      "step:  271000 correct_count:  21027 except_count 6956\n",
      "step:  272000 correct_count:  21159 except_count 6977\n",
      "step:  273000 correct_count:  21295 except_count 6986\n",
      "step:  274000 correct_count:  21405 except_count 7001\n",
      "step:  275000 correct_count:  21544 except_count 7001\n",
      "step:  276000 correct_count:  21666 except_count 7016\n",
      "step:  277000 correct_count:  21752 except_count 7031\n",
      "step:  278000 correct_count:  21835 except_count 7076\n",
      "step:  279000 correct_count:  21915 except_count 7091\n",
      "step:  280000 correct_count:  22007 except_count 7106\n",
      "step:  281000 correct_count:  22096 except_count 7136\n",
      "step:  282000 correct_count:  22199 except_count 7196\n",
      "step:  283000 correct_count:  22287 except_count 7226\n",
      "step:  284000 correct_count:  22384 except_count 7241\n",
      "step:  285000 correct_count:  22462 except_count 7286\n",
      "step:  286000 correct_count:  22552 except_count 7317\n",
      "step:  287000 correct_count:  22658 except_count 7332\n",
      "step:  288000 correct_count:  22750 except_count 7362\n",
      "step:  289000 correct_count:  22856 except_count 7392\n",
      "step:  290000 correct_count:  22947 except_count 7452\n",
      "step:  291000 correct_count:  23044 except_count 7467\n",
      "step:  292000 correct_count:  23130 except_count 7497\n",
      "step:  293000 correct_count:  23223 except_count 7512\n",
      "step:  294000 correct_count:  23323 except_count 7542\n",
      "step:  295000 correct_count:  23401 except_count 7557\n",
      "step:  296000 correct_count:  23507 except_count 7602\n",
      "step:  297000 correct_count:  23574 except_count 7647\n",
      "step:  298000 correct_count:  23651 except_count 7736\n",
      "step:  299000 correct_count:  23743 except_count 7736\n",
      "step:  300000 correct_count:  23831 except_count 7751\n",
      "step:  301000 correct_count:  23934 except_count 7781\n",
      "step:  302000 correct_count:  24018 except_count 7842\n",
      "step:  303000 correct_count:  24092 except_count 7902\n",
      "step:  304000 correct_count:  24154 except_count 8007\n",
      "step:  305000 correct_count:  24254 except_count 8039\n",
      "step:  306000 correct_count:  24335 except_count 8069\n",
      "step:  307000 correct_count:  24434 except_count 8069\n",
      "step:  308000 correct_count:  24502 except_count 8159\n",
      "step:  309000 correct_count:  24591 except_count 8205\n",
      "step:  310000 correct_count:  24656 except_count 8268\n",
      "step:  311000 correct_count:  24748 except_count 8313\n",
      "step:  312000 correct_count:  24809 except_count 8371\n",
      "step:  313000 correct_count:  24892 except_count 8431\n",
      "step:  314000 correct_count:  24990 except_count 8476\n",
      "step:  315000 correct_count:  25067 except_count 8536\n",
      "step:  316000 correct_count:  25166 except_count 8566\n",
      "step:  317000 correct_count:  25261 except_count 8574\n",
      "step:  318000 correct_count:  25333 except_count 8604\n",
      "step:  319000 correct_count:  25420 except_count 8652\n",
      "step:  320000 correct_count:  25495 except_count 8712\n",
      "step:  321000 correct_count:  25587 except_count 8757\n",
      "step:  322000 correct_count:  25663 except_count 8823\n",
      "step:  323000 correct_count:  25755 except_count 8883\n",
      "step:  324000 correct_count:  25830 except_count 8958\n",
      "step:  325000 correct_count:  25917 except_count 8988\n",
      "step:  326000 correct_count:  26004 except_count 9063\n",
      "step:  327000 correct_count:  26093 except_count 9123\n",
      "step:  328000 correct_count:  26181 except_count 9266\n",
      "step:  329000 correct_count:  26260 except_count 9281\n",
      "step:  330000 correct_count:  26342 except_count 9311\n",
      "step:  331000 correct_count:  26439 except_count 9371\n",
      "step:  332000 correct_count:  26499 except_count 9662\n",
      "correct_count:  26499 except_count 9780\n"
     ]
    }
   ],
   "source": [
    "from KoHis import KoHisQnA\n",
    "qa = KoHisQnA()  \n",
    "\n",
    "count = 0\n",
    "\n",
    "correct_quiz = []\n",
    "correct_count = 0\n",
    "\n",
    "except_list = []\n",
    "except_count = 0\n",
    "\n",
    "for i in tqdm_notebook(range(332430)):\n",
    "    try:\n",
    "        tuple_answer = qa.do_ask_to_model(all_list[i]['generated_question'], all_list[i]['doc'])\n",
    "        predict_answer = tuple_answer[2]\n",
    "    except: ## 문서의 길이가 너무 길경우\n",
    "        except_list.append(all_list[i])\n",
    "        except_count += 1\n",
    "    if \"#\" in predict_answer:\n",
    "        predict_answer = predict_answer.replace(\"#\", \"\")\n",
    "    if all_list[i]['answer'] == predict_answer:\n",
    "\n",
    "        correct_quiz.append(all_list[i])\n",
    "        correct_count += 1\n",
    "    if count % 1000 == 0:\n",
    "       print(\"step: \", count, \"correct_count: \", correct_count, \"except_count\", except_count)\n",
    "    count += 1\n",
    "        \n",
    "print(\"correct_count: \", correct_count, \"except_count\", except_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc': '가군은 삼국시대 고구려 장안성 축성의 책임을 맡았던 관리. 가군은 생몰년 미상. 1964년에 평양 중구역 남문동에서 발견된 내성의 성벽돌에 새겨져 있는 명문에 따르면 가군은 성곽 축조시에 소형 관등을 가졌으며, 장안성 내성 중 일정 거리의 성벽 축조를 감독하였음을 알 수 있다.고구려 장안성은 522년에 쌓았고 589년에 장안성으로 천도하였다. 따라서 그의 활동 시기는 대략 6세기 후반 평원왕대로 추정된다. 또 명문의 괘루개절은 가군의 출신지나 직명으로 볼 수 있다.괘루는 고구려 5부의 하나인 계루부의 다른 표기로 볼 수도 있으나 아직 단정하기는 어려우며, 개절은 관직명의 일종으로 추정되고 있다. 이 명문이 새겨진 성벽돌은 현재 인민대학습당 내부를 지나는 성벽의 원위치에 그대로 있다.',\n",
       " 'answer': '가군',\n",
       " 'generated_question': '522년 고구려 장안성 축성에 책임을 맡은 관리들은 누구인가?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_quiz[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = []\n",
    "answers_list = []\n",
    "question_list = []\n",
    "\n",
    "for data in correct_quiz:\n",
    "    context_list.append(data['doc'])\n",
    "    answers_list.append(data['answer'])\n",
    "    question_list.append(data['generated_question'])\n",
    "            \n",
    "data = {\n",
    "        'context':context_list,\n",
    "        'answers': answers_list,\n",
    "        'question':question_list\n",
    "    }\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('History_train_V1.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "acf6e4abe0c500b723931f115c3e38565af0a1699b33a74e0337a5d37d48ad31"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('historyQA': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
