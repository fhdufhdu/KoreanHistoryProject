{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm_notebook\n",
    "from transformers import GPT2LMHeadModel, get_linear_schedule_with_warmup\n",
    "from korquad_qg.config import QGConfig\n",
    "from korquad_qg.utils import TqdmLoggingHandler\n",
    "from typing import List, NamedTuple, Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 코쿼드 데이터셋을 이용한 GPT2 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('korquad.json', 'r') as f:\n",
    "\n",
    "    json_data = json.load(f)\n",
    "\n",
    "context_list = []\n",
    "answers_list = []\n",
    "question_list = []\n",
    "\n",
    "for data in json_data['data']:\n",
    "    for sub_data in data['paragraphs']:\n",
    "        context = sub_data['context']\n",
    "        for qa in sub_data['qas']:\n",
    "            context_list.append(context)\n",
    "            answers_list.append(qa['answers'][0]['text'])\n",
    "            question_list.append(qa['question'])\n",
    "            if len(qa['answers']) > 1:\n",
    "                print(qa['answers'])\n",
    "                \n",
    "data = {\n",
    "        'context':context_list,\n",
    "        'answers': answers_list,\n",
    "        'question':question_list\n",
    "    }\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('KorQuad_train_V1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토크나이저 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_f = open(\"sentences.txt\", 'r')\n",
    "all_dict = []\n",
    "max_len = 0\n",
    "count = 0\n",
    "for line in tqdm_notebook(txt_f.readlines()):\n",
    "    if '\\n' in line:\n",
    "        line = line.replace('\\n', '')\n",
    "    if '##' in line:\n",
    "        line = line.replace('##', '')\n",
    "    if len(line) > max_len:\n",
    "        max_len = len(line)\n",
    "    all_dict.append(line)\n",
    "    count += 1\n",
    "print(max_len)\n",
    "\n",
    "\n",
    "f = open(\"data_save_test.txt\", 'w')\n",
    "for i in all_dict:\n",
    "    f.write(i+'\\n')\n",
    "f.close()\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "# Then train it!\n",
    "\n",
    "tokenizer.train_from_iterator([all_dict], vocab_size=300000, min_frequency=1, limit_alphabet=100000)\n",
    "tokenizer.save(\"./history_tokenizer.json\",pretty=True)\n",
    "tokenizer.save_model(directory='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_f = open(\"sentences.txt\", 'r')\n",
    "max_len = 512\n",
    "count = 0\n",
    "tok = PreTrainedTokenizerFast(tokenizer_file='Tokenizer/History/history_tokenizer.json')\n",
    "for line in tqdm_notebook(txt_f.readlines()):\n",
    "    if '\\n' in line:\n",
    "        line = line.replace('\\n', '')\n",
    "    if '##' in line:\n",
    "        line = line.replace('##', '')\n",
    "    if len(tok.tokenize(line)) + 7 > max_len:\n",
    "        max_len = len(line)\n",
    "        count += 1\n",
    "print(max_len, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_TKN = '<q>'\n",
    "A_TKN = '<a>'\n",
    "BOS = '<s>'\n",
    "EOS = '</s>'\n",
    "MASK = '<mask>'\n",
    "C_TKN = '<c>'\n",
    "PAD = '<pad>'\n",
    "TOKENIZER = PreTrainedTokenizerFast(tokenizer_file='Tokenizer/tokenizer.json',\n",
    "            bos_token=BOS, eos_token=EOS, unk_token='<unk>', \n",
    "            pad_token=PAD, mask_token=MASK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 셋 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPTDecodingInputType = Tuple[torch.Tensor, torch.Tensor]\n",
    "GPTInputsType = Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "GPTFeaturesType = Tuple[List[int], List[float], List[int]]\n",
    "\n",
    "class QAExample(NamedTuple):\n",
    "    context: str\n",
    "    answer: str\n",
    "    question: Optional[str] = None\n",
    "\n",
    "def load_korquad_dataset(dataset_path: str) -> List[QAExample]:\n",
    "    korquad = [pd.read_csv(dataset_path)]\n",
    "    max_len = 512\n",
    "    examples = []\n",
    "    for document in korquad:\n",
    "        for i in tqdm_notebook(range(len(document))):\n",
    "            if len(TOKENIZER.tokenize(document[\"context\"][i])) + 10 <= max_len:\n",
    "                example = QAExample(document[\"context\"][i], document[\"answers\"][i], document[\"question\"][i])\n",
    "                examples.append(example)\n",
    "        \n",
    "    return examples\n",
    "    \n",
    "def dynamic_padding_collate_fn(features: List[GPTFeaturesType]) -> GPTInputsType:\n",
    "    max_seq_len = max([len(feature[0]) for feature in features])\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "\n",
    "    for feature in features:\n",
    "        padded_input_ids = feature[0] + [0] * (max_seq_len - len(feature[0]))\n",
    "        padded_attention_mask = feature[1] + [0.0] * (max_seq_len - len(feature[1]))\n",
    "        padded_labels = feature[2] + [-100] * (max_seq_len - len(feature[2]))\n",
    "\n",
    "        input_ids.append(padded_input_ids)\n",
    "        attention_mask.append(padded_attention_mask)\n",
    "        labels.append(padded_labels)\n",
    "\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryQGDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        examples: List[QAExample],\n",
    "        tokenizer: SentencePieceBPETokenizer,\n",
    "        max_sequence_length: int,\n",
    "        is_train: bool = True,\n",
    "    ) -> None:\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "        self.sos_token = tokenizer.convert_tokens_to_ids(\"<s>\")\n",
    "        self.eos_token = tokenizer.convert_tokens_to_ids(\"</s>\")\n",
    "        self.question_prefix_tokens = tokenizer.convert_tokens_to_ids('<q>')\n",
    "\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __getitem__(self, index: int) -> GPTFeaturesType:\n",
    "        example = self.examples[index]\n",
    "\n",
    "        context_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"<c>{example.context}\"))\n",
    "        answer_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"<a>{example.answer}\"))\n",
    "        question_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"{example.question}\"))\n",
    "        \n",
    "        # [SOS] + 문맥:CONTEXT + 정답:ANSWER + 질문:\n",
    "        conditional_tokens_len = 1 + len(context_tokens) + len(answer_tokens) + 1\n",
    "        # QUESTION + [EOS]\n",
    "        post_tokens_len = len(question_tokens) + 1\n",
    "\n",
    "        if conditional_tokens_len + post_tokens_len > self.max_sequence_length:\n",
    "            available_seq_len = (\n",
    "                self.max_sequence_length - conditional_tokens_len - post_tokens_len + len(context_tokens)\n",
    "            )\n",
    "            context_tokens = context_tokens[:available_seq_len]\n",
    "\n",
    "        conditional_tokens = [self.sos_token] + context_tokens + answer_tokens + [self.question_prefix_tokens]\n",
    "        post_tokens = question_tokens + [self.eos_token]\n",
    "        input_ids = conditional_tokens + post_tokens\n",
    "\n",
    "        labels = input_ids if self.is_train else ([-100] * len(conditional_tokens)) + post_tokens\n",
    "        attention_mask = [1.0] * len(input_ids)\n",
    "\n",
    "        assert len(input_ids) <= self.max_sequence_length\n",
    "\n",
    "        return input_ids, attention_mask, labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_logger(output_dir: str):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"[%(asctime)s] %(message)s\")\n",
    "\n",
    "    file_handler = logging.FileHandler(os.path.join(output_dir, \"train.log\"))\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    handler = TqdmLoggingHandler()\n",
    "    handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\"))\n",
    "    logger.addHandler(handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate(\n",
    "    model: GPT2LMHeadModel,\n",
    "    dev_dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    logger: logging.Logger,\n",
    "    global_step: int,\n",
    "):\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    for batch_data in tqdm_notebook(dev_dataloader, desc=\"[EVAL]\"):\n",
    "        with torch.no_grad():\n",
    "            input_ids, attention_mask, labels = tuple(value.to(device) for value in batch_data)\n",
    "            model_outputs = model.forward(input_ids, attention_mask=attention_mask, labels=labels, return_dict=True)\n",
    "            loss_list.append(model_outputs.loss.item())\n",
    "\n",
    "    mean_loss = np.mean(loss_list)\n",
    "    logger.info(f\"[EVAL] global_step:{global_step} loss:{mean_loss:.4f} perplexity:{math.exp(mean_loss):.4f}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "# Initializing a model from the configuration\n",
    "model = GPT2LMHeadModel(config=GPT2Config.from_json_file('config.json'))\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = QGConfig()\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file='Tokenizer/tokenizer.json',\n",
    "            bos_token=BOS, eos_token=EOS, unk_token='<unk>', \n",
    "            pad_token=PAD, mask_token=MASK)\n",
    "\n",
    "logger = _create_logger(output_dir=config.output_dir)\n",
    "logger.info(\"============================\")\n",
    "for key, value in config._asdict().items():\n",
    "    logger.info(f\"{key:30}:{value}\")\n",
    "logger.info(\"============================\")\n",
    "torch.manual_seed(config.random_seed)\n",
    "\n",
    "logger.info(\"loading train dataset\")\n",
    "train_examples = load_korquad_dataset(config.train_dataset)\n",
    "train_dataset = HistoryQGDataset(train_examples, tokenizer, config.max_sequence_length)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, 16, shuffle=True, collate_fn=dynamic_padding_collate_fn)\n",
    "\n",
    "logger.info(\"loading dev dataset\")\n",
    "dev_examples = load_korquad_dataset(config.dev_dataset)\n",
    "dev_dataset = HistoryQGDataset(dev_examples, tokenizer, config.max_sequence_length, is_train=False)\n",
    "dev_dataloader = DataLoader(dev_dataset, 16, collate_fn=dynamic_padding_collate_fn)\n",
    "\n",
    "#model 생성\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "print(model.transformer.wte.weight.shape[0], len(tokenizer.vocab))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=config.lr)\n",
    "total_steps = len(train_dataloader) * config.epochs\n",
    "warmup_steps = int(total_steps * config.warmup_ratio)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "loss_list_between_log_interval = []\n",
    "for epoch_id in range(config.epochs):\n",
    "    for step_index, batch_data in tqdm_notebook(\n",
    "            enumerate(train_dataloader), f\"[TRAIN] EP:{epoch_id}\", total=len(train_dataloader)\n",
    "    ):\n",
    "        global_step = len(train_dataloader) * epoch_id + step_index + 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        token_ids, attention_mask, labels = tuple(value.to(device) for value in batch_data)\n",
    "        model_outputs = model.forward(token_ids, attention_mask=attention_mask, labels=labels, return_dict=True)\n",
    "        model_outputs.loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # for logging\n",
    "        loss_list_between_log_interval.append(model_outputs.loss.item())\n",
    "\n",
    "        if global_step % config.train_log_interval == 0:\n",
    "            mean_loss = np.mean(loss_list_between_log_interval)\n",
    "            logger.info(\n",
    "                    f\"EP:{epoch_id} global_step:{global_step} \"\n",
    "                    f\"loss:{mean_loss:.4f} perplexity:{math.exp(mean_loss):.4f}\"\n",
    "                )\n",
    "            loss_list_between_log_interval.clear()\n",
    "            \n",
    "        if global_step % config.validation_interval == 0:\n",
    "                _validate(model, dev_dataloader, device, logger, global_step)\n",
    "                \n",
    "        if global_step % config.save_interval == 0:\n",
    "            state_dict = model.state_dict()\n",
    "            model_path = os.path.join(config.output_dir, f\"gpt2_step_{global_step}.pth\")\n",
    "            torch.save(state_dict, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGDecodingDataset(HistoryQGDataset):\n",
    "    def __getitem__(self, index: int) -> GPTDecodingInputType:\n",
    "        example = self.examples[index]\n",
    "\n",
    "        context_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"<c>{example.context}\"))\n",
    "        answer_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"<a>{example.answer}\"))\n",
    "\n",
    "        # [SOS] + CONTEXT + ANSWER + 정답:\n",
    "        conditional_tokens_len = 1 + len(context_tokens) + len(answer_tokens) + 1\n",
    "        # ANSWER_SEQ + [EOS]\n",
    "        post_tokens_len = MAX_QUESTION_SPACE + 1\n",
    "        if conditional_tokens_len + post_tokens_len > self.max_sequence_length:\n",
    "            available_seq_len = (\n",
    "                self.max_sequence_length - conditional_tokens_len - post_tokens_len + len(context_tokens)\n",
    "            )\n",
    "            context_tokens = context_tokens[:available_seq_len]\n",
    "\n",
    "        input_ids = [self.sos_token] + context_tokens + answer_tokens + [self.question_prefix_tokens]\n",
    "        attention_mask = [1.0] * len(input_ids)\n",
    "\n",
    "        return torch.tensor(input_ids), torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/110810 [00:00<00:01, 67080.62it/s]\n",
      "generate: 100%|██████████| 20/20 [00:07<00:00,  2.70it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 11921.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가군이 장안성에 도읍을 옮긴 시기는?\n",
      "가군이 장안성에 도읍을 옮긴 시기는?\n",
      "가군이 쌓은 성은?\n",
      "가군이 성내의 성벽축조를 감독한 곳은?\n",
      "가군이 장안성 내에서 쌓은 성은?\n",
      "가군이 쌓은 성은?\n",
      "가군이 장안성을 축성할 때 사용한 관등은?\n",
      "가군이 쌓은 성벽은 언제 쌓았는가?\n",
      "가군이 장안성 축조시에 무엇을 하였나?\n",
      "가군이 장안성 축조 시 사용한 관등은?\n",
      "가군이 장안성에 쌓은 성벽은?\n",
      "가군이 성 안에 세운 관등은?\n",
      "가군이 쌓은 성문은?\n",
      "가군의 소속은?\n",
      "가군이 장안성 축조시 사용한 관등은?\n",
      "가귀는 화엄경의강과 심원장은 어디인가?\n",
      "가귀가 화엄경에서 화엄경을 알았던 것은?\n",
      "가귀가 화엄경에서 화엄경의, 심원장 등을 지은 것은?\n",
      "가귀가 화엄경의강에서 화엄경을 해설한 책은?\n",
      "가귀가 화엄경의강에서 화엄경을 알았던 것은?\n",
      "가귀는 화엄경의강을 저술한 승려인가?\n",
      "가귀가 화엄경의강을 지은 것은?\n",
      "가귀가 화엄경에서 화엄경을 해설한 책은?\n",
      "가귀가 화엄경의, 화엄경의, 심원장은 어디인가?\n",
      "가귀는 화엄경의강을 누구의 제자인가?\n",
      "가귀가 화엄경의강에서 화엄경을 알았던 것은?\n",
      "가귀가 화엄경의강에서 무엇을 알았는가?\n",
      "가귀는 화엄경의강에서 무엇을 알았는가?\n",
      "가귀가 화엄경의강을 저술한 승려는?\n",
      "가귀가 화엄경의강에서 화엄경을 해설한 책은?\n",
      "가야지방 유이민은 어느 지역에 거주하였는가?\n",
      "가라포고이의 이름은 무엇인가?\n",
      "가라포고이의 본명은?\n",
      "가야지역의 유이민으로 이름이 붙여진 나라는?\n",
      "가야지방 유이민은 어느 지역에 살았는가?\n",
      "가야지방 유이민의 이름은 무엇인가?\n",
      "가야지방의 유민으로 이름이 붙여진 나라는?\n",
      "가야의 유민들 중 가가야로 알려진 사람은?\n",
      "가야지방 유이민의 이름은 무엇인가?\n",
      "가야의 유민 중 신라인과 왜 사이에서 정상적인 통교되지 않은 사람은?\n",
      "가야지방 유이민은 어느 지역에 거주하였는가?\n",
      "신라에서 귀화한 신라인의 이름은 무엇인가?\n",
      "가야의 유민으로 알려진 신라인은?\n",
      "가야지방 유이민의 이름은 무엇인가?\n",
      "백제 유민인 가라포고이의 이름은 무엇인가?\n",
      "백제 부흥운동 때 유민들이 세운 고구려의 사신의 이름은?\n",
      "고구려 멸망후 백제의 부흥운동에 참여한 귀족은?\n",
      "고구려 멸망후 백제의 부흥운동에 참여한 귀족은?\n",
      "고구려 멸망후 백제의 부흥운동에 참여한 귀족은?\n",
      "신라 멸망 후 백제국 고관의 관직을 맡았던 사람은?\n",
      "고구려 멸망후 신라국 고관은 어디로 파견되었는가?\n",
      "고구려 멸망후 신라국 고관에 파견되었던 인물은?\n",
      "신라 멸망 후 백제의 부흥운동에 참여한 인물은?\n",
      "고구려 멸망후 백제국 고관의 직위는?\n",
      "고구려 부흥운동에 참여한 귀족은?\n",
      "고구려 멸망후 백제의 부흥운동에 참여한 귀족은?\n",
      "고구려 멸망후 고구려의 부흥운동을 이끈 사람은?\n",
      "신라 멸망 후 신라국 고관에 파견된 인물은?\n",
      "신라 멸망 후 백제의 부흥운동에 참여한 인물은?\n",
      "신라 멸망 후 백제의 부흥운동에 참여한 사람은?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import json\n",
    "\n",
    "from korquad_qg.config import QGConfig\n",
    "from korquad_qg.dataset import MAX_QUESTION_SPACE, MIN_QUESTION_SPACE, QAExample, dynamic_padding_collate_fn\n",
    "\n",
    "\n",
    "config = QGConfig()\n",
    "#args = parser.parse_args()\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file='Tokenizer/tokenizer.json',\n",
    "         bos_token=BOS, eos_token=EOS, unk_token='<unk>', \n",
    "         pad_token=PAD, mask_token=MASK)\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "model.load_state_dict(torch.load('outputs/V2/gpt2_step_12000.pth', map_location=\"cpu\"))\n",
    "\n",
    "#tokenizer = PreTrainedTokenizerFast(tokenizer_file='Tokenizer/History_Korquad_Tokenizer/history_Korquad_tokenizer.json',\n",
    "#            bos_token=BOS, eos_token=EOS, unk_token='<unk>', \n",
    "#            pad_token=PAD, mask_token=MASK)\n",
    "#model = GPT2LMHeadModel(config=GPT2Config.from_json_file('config.json'))\n",
    "#model.load_state_dict(torch.load('outputs/gpt2_step_150000.pth', map_location=\"cpu\"))\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "Q_TKN = '<q>'\n",
    "A_TKN = '<a>'\n",
    "BOS = '<s>'\n",
    "EOS = '</s>'\n",
    "MASK = '<mask>'\n",
    "C_TKN = '<c>'\n",
    "PAD = '<pad>'\n",
    "\n",
    "examples = []\n",
    "\n",
    "with open('as_set.json') as f:\n",
    "    data = json.load(f)\n",
    "count = 0\n",
    "for i in tqdm(data['data']):\n",
    "    example = QAExample(i['doc'], \"\")\n",
    "    examples.append(example)\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "dataset = QGDecodingDataset(examples, tokenizer, 512)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, 1)\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "generated_results = []\n",
    "\n",
    "for i, batch in tqdm(enumerate(dataloader), desc=\"generate\", total=len(dataloader)):\n",
    "    input_ids, attention_mask = tuple(v.to(device) for v in batch)\n",
    "    origin_seq_len = input_ids.size(-1)\n",
    "\n",
    "    decoded_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=origin_seq_len + MAX_QUESTION_SPACE,\n",
    "        min_length=origin_seq_len + MIN_QUESTION_SPACE,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        do_sample=True,\n",
    "        num_beams=4,\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_return_sequences=3\n",
    "    )\n",
    "\n",
    "    for decoded_tokens in decoded_sequences.tolist():\n",
    "\n",
    "        decoded_question_text = tokenizer.decode(decoded_tokens[origin_seq_len:])\n",
    "        #decoded_question_text = tokenizer.decode(decoded_tokens)\n",
    "        #print(decoded_question_text)\n",
    "        decoded_question_text = decoded_question_text.split(\"</s>\")[0].replace(\"<c>\", \"\")\n",
    "        generated_results.append(\n",
    "            (examples[i].context, examples[i].answer, examples[i].question, decoded_question_text)\n",
    "        )\n",
    "with open(\"article_qg.tsv\", \"w\") as f:\n",
    "    \n",
    "    for context, answer, question, generated_question in tqdm(generated_results):\n",
    "        print(generated_question)\n",
    "        f.write(f\"문맥\\t{context}\\n\")\n",
    "        f.write(f\"답변\\t{answer}\\n\")\n",
    "        f.write(f\"생성된 질문\\t{generated_question}\\n\")\n",
    "        if question is not None:\n",
    "             f.write(f\"실제 질문\\t{question}\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dbce115c9e272f3a95a5e3442ce986783d75184bcb86cea9cb2c12b886171c29"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
