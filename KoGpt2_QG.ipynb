{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PreTrainedTokenizerFast' from 'transformers' (/home/hsoh0423/anaconda3/envs/historyQA/lib/python3.7/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11652/2202418223.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedTokenizerFast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PreTrainedTokenizerFast' from 'transformers' (/home/hsoh0423/anaconda3/envs/historyQA/lib/python3.7/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import math\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm_notebook\n",
    "from transformers import GPT2LMHeadModel, get_linear_schedule_with_warmup\n",
    "from korquad_qg.config import QGConfig\n",
    "from korquad_qg.utils import TqdmLoggingHandler\n",
    "from typing import List, NamedTuple, Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 셋 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_TKN = '<q>'\n",
    "A_TKN = '<a>'\n",
    "BOS = '<s>'\n",
    "EOS = '</s>'\n",
    "MASK = '<mask>'\n",
    "C_TKN = '<c>'\n",
    "PAD = '<pad>'\n",
    "TOKENIZER = PreTrainedTokenizerFast(tokenizer_file='Tokenizer/tokenizer.json',\n",
    "            bos_token=BOS, eos_token=EOS, unk_token='<unk>', \n",
    "            pad_token=PAD, mask_token=MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPTDecodingInputType = Tuple[torch.Tensor, torch.Tensor]\n",
    "GPTInputsType = Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "GPTFeaturesType = Tuple[List[int], List[float], List[int]]\n",
    "\n",
    "class QAExample(NamedTuple):\n",
    "    context: str\n",
    "    answer: str\n",
    "    question: Optional[str] = None\n",
    "\n",
    "def load_korquad_dataset(dataset_path: str) -> List[QAExample]:\n",
    "    korquad = [pd.read_csv(dataset_path)]\n",
    "    max_len = 512\n",
    "    examples = []\n",
    "    for document in korquad:\n",
    "        for i in tqdm_notebook(range(len(document))):\n",
    "            if len(TOKENIZER.tokenize(document[\"context\"][i])) + 10 <= max_len:\n",
    "                example = QAExample(document[\"context\"][i], document[\"answers\"][i], document[\"question\"][i])\n",
    "                examples.append(example)\n",
    "        \n",
    "    return examples\n",
    "    \n",
    "def dynamic_padding_collate_fn(features: List[GPTFeaturesType]) -> GPTInputsType:\n",
    "    max_seq_len = max([len(feature[0]) for feature in features])\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "\n",
    "    for feature in features:\n",
    "        padded_input_ids = feature[0] + [0] * (max_seq_len - len(feature[0]))\n",
    "        padded_attention_mask = feature[1] + [0.0] * (max_seq_len - len(feature[1]))\n",
    "        padded_labels = feature[2] + [-100] * (max_seq_len - len(feature[2]))\n",
    "\n",
    "        input_ids.append(padded_input_ids)\n",
    "        attention_mask.append(padded_attention_mask)\n",
    "        labels.append(padded_labels)\n",
    "\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryQGDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        examples: List[QAExample],\n",
    "        tokenizer: SentencePieceBPETokenizer,\n",
    "        max_sequence_length: int,\n",
    "        is_train: bool = True,\n",
    "    ) -> None:\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "        self.sos_token = tokenizer.convert_tokens_to_ids(\"<s>\")\n",
    "        self.eos_token = tokenizer.convert_tokens_to_ids(\"</s>\")\n",
    "        self.question_prefix_tokens = tokenizer.convert_tokens_to_ids('<q>')\n",
    "\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __getitem__(self, index: int) -> GPTFeaturesType:\n",
    "        example = self.examples[index]\n",
    "\n",
    "        context_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"<c>{example.context}\"))\n",
    "        answer_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"<a>{example.answer}\"))\n",
    "        question_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"{example.question}\"))\n",
    "        \n",
    "        # [SOS] + 문맥:CONTEXT + 정답:ANSWER + 질문:\n",
    "        conditional_tokens_len = 1 + len(context_tokens) + len(answer_tokens) + 1\n",
    "        # QUESTION + [EOS]\n",
    "        post_tokens_len = len(question_tokens) + 1\n",
    "\n",
    "        if conditional_tokens_len + post_tokens_len > self.max_sequence_length:\n",
    "            available_seq_len = (\n",
    "                self.max_sequence_length - conditional_tokens_len - post_tokens_len + len(context_tokens)\n",
    "            )\n",
    "            context_tokens = context_tokens[:available_seq_len]\n",
    "\n",
    "        conditional_tokens = [self.sos_token] + context_tokens + answer_tokens + [self.question_prefix_tokens]\n",
    "        post_tokens = question_tokens + [self.eos_token]\n",
    "        input_ids = conditional_tokens + post_tokens\n",
    "\n",
    "        labels = input_ids if self.is_train else ([-100] * len(conditional_tokens)) + post_tokens\n",
    "        attention_mask = [1.0] * len(input_ids)\n",
    "\n",
    "        assert len(input_ids) <= self.max_sequence_length\n",
    "\n",
    "        return input_ids, attention_mask, labels\n",
    "        #return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(labels)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 퀴즈 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGDecodingDataset(HistoryQGDataset):\n",
    "    def __getitem__(self, index: int) -> GPTDecodingInputType:\n",
    "        example = self.examples[index]\n",
    "\n",
    "        context_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"<c>{example.context}\"))\n",
    "        answer_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"<a>{example.answer}\"))\n",
    "\n",
    "        # [SOS] + CONTEXT + ANSWER + 정답:\n",
    "        conditional_tokens_len = 1 + len(context_tokens) + len(answer_tokens) + 1\n",
    "        # ANSWER_SEQ + [EOS]\n",
    "        post_tokens_len = MAX_QUESTION_SPACE + 1\n",
    "        if conditional_tokens_len + post_tokens_len > self.max_sequence_length:\n",
    "            available_seq_len = (\n",
    "                self.max_sequence_length - conditional_tokens_len - post_tokens_len + len(context_tokens)\n",
    "            )\n",
    "            context_tokens = context_tokens[:available_seq_len]\n",
    "\n",
    "        input_ids = [self.sos_token] + context_tokens + answer_tokens + [self.question_prefix_tokens]\n",
    "        attention_mask = [1.0] * len(input_ids)\n",
    "        return torch.tensor(input_ids), torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/110810 [00:00<00:01, 106539.81it/s]\n",
      "generate: 100%|██████████| 20/20 [00:04<00:00,  4.22it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 12933.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고구려의 장안성 축성의 책임자였던 사람은?\n",
      "가군의 출신지나 직명으로 볼 수 있는 것은?\n",
      "고구려 5부의 하나인 계루부의 다른 표기는?\n",
      "가군의 출신지는?\n",
      "가군의 출신지는?\n",
      "신라에서 화엄경의강을 지은 사람은?\n",
      "가귀가 지은 화엄경의강은?\n",
      "가귀는 승려인가?\n",
      "가귀는 어느 왕때 승려인가?\n",
      "가귀는 승려인가?\n",
      "가라포고이가 귀화한 사람은?\n",
      "귀화한 신라인은 누구인가?\n",
      "가라포고이가 귀화한 시기는?\n",
      "가라포고이가 귀화한 나라는?\n",
      "가라포고이가 일본에서 귀화한 것은 무엇인가?\n",
      "고구려 부흥운동 당시 상부대상을 역임한 귀족은?\n",
      "고구려 부흥운동을 지휘한 사람은?\n",
      "고구려 멸망 후 부흥운동을 펼친 나라는?\n",
      "고구려 부흥운동 당시 상부대상을 역임한 귀족은?\n",
      "고구려 부흥운동이 시작된 시기는?\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import json\n",
    "\n",
    "from korquad_qg.config import QGConfig\n",
    "from korquad_qg.dataset import MAX_QUESTION_SPACE, MIN_QUESTION_SPACE, QAExample, dynamic_padding_collate_fn\n",
    "\n",
    "Q_TKN = '<q>'\n",
    "A_TKN = '<a>'\n",
    "BOS = '<s>'\n",
    "EOS = '</s>'\n",
    "MASK = '<mask>'\n",
    "C_TKN = '<c>'\n",
    "PAD = '<pad>'\n",
    "\n",
    "config = QGConfig()\n",
    "#args = parser.parse_args()\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file='Tokenizer/tokenizer.json',\n",
    "         bos_token=BOS, eos_token=EOS, unk_token='<unk>', \n",
    "         pad_token=PAD, mask_token=MASK)\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "model.load_state_dict(torch.load('outputs/gpt2_step_20000.pth'))\n",
    "#model.load_state_dict(torch.load('outputs/gpt2_step_17000.pth'))\n",
    "\n",
    "#tokenizer = PreTrainedTokenizerFast(tokenizer_file='Tokenizer/History_Korquad_Tokenizer/history_Korquad_tokenizer.json',\n",
    "#            bos_token=BOS, eos_token=EOS, unk_token='<unk>', \n",
    "#            pad_token=PAD, mask_token=MASK)\n",
    "#model = GPT2LMHeadModel(config=GPT2Config.from_json_file('config.json'))\n",
    "#model.load_state_dict(torch.load('outputs/gpt2_step_150000.pth', map_location=\"cpu\"))\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "examples = []\n",
    "\n",
    "with open('data/as_set.json') as f:\n",
    "    data = json.load(f)\n",
    "count = 0\n",
    "for i in tqdm(data['data']):\n",
    "    example = QAExample(i['doc'], i['answer'])\n",
    "    examples.append(example)\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "dataset = QGDecodingDataset(examples, tokenizer, 512)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, 1)\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "generated_results = []\n",
    "\n",
    "for i, batch in tqdm(enumerate(dataloader), desc=\"generate\", total=len(dataloader)):\n",
    "    input_ids, attention_mask = tuple(v.to(device) for v in batch)\n",
    "    origin_seq_len = input_ids.size(-1)\n",
    "\n",
    "    decoded_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=origin_seq_len + MAX_QUESTION_SPACE,\n",
    "        min_length=origin_seq_len + MIN_QUESTION_SPACE,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        num_beams=5,\n",
    "        repetition_penalty=2.0,\n",
    "        #no_repeat_ngram_size=3\n",
    "        #num_return_sequences=1\n",
    "    )\n",
    "\n",
    "    for decoded_tokens in decoded_sequences.tolist():\n",
    "\n",
    "        decoded_question_text = tokenizer.decode(decoded_tokens[origin_seq_len:])\n",
    "        #decoded_question_text = tokenizer.decode(decoded_tokens)\n",
    "        #print(decoded_question_text)\n",
    "        decoded_question_text = decoded_question_text.split(\"</s>\")[0].replace(\"<c>\", \"\")\n",
    "        generated_results.append(\n",
    "            (examples[i].context, examples[i].answer, examples[i].question, decoded_question_text)\n",
    "        )\n",
    "        \n",
    "context_list = []\n",
    "answer_list = []\n",
    "question_list = []\n",
    "    \n",
    "for context, answer, question, generated_question in tqdm(generated_results):\n",
    "        print(generated_question)\n",
    "        context_list.append(context)\n",
    "        answer_list.append(answer) \n",
    "        question_list.append(generated_question)\n",
    "        \n",
    "generated_data = {\n",
    "        'context':context_list,\n",
    "        'answers': answer_list,\n",
    "        'question':question_list\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame(generated_data)\n",
    "df.to_csv('Generated_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dbce115c9e272f3a95a5e3442ce986783d75184bcb86cea9cb2c12b886171c29"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
