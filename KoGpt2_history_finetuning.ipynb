{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import logging\n",
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<s>'\n",
    "EOS = '</s>'\n",
    "MASK = '<mask>'\n",
    "PAD = '<pad>'\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file='Tokenizer/tokenizer.json',\n",
    "            bos_token=BOS, eos_token=EOS, unk_token='<unk>', \n",
    "            pad_token=PAD, mask_token=MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryDataset(Dataset):\n",
    "    def __init__(self, historyData,tokenizer):\n",
    "        self.data = historyData\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = 512\n",
    "        self.sos_token = tokenizer.convert_tokens_to_ids(\"<s>\")\n",
    "        self.eos_token = tokenizer.convert_tokens_to_ids(\"</s>\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        item = self.data[index]\n",
    "        context_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(f\"{item}\"))\n",
    "        conditional_tokens_len = 1 + len(context_tokens) + 1\n",
    "        \n",
    "        if conditional_tokens_len > self.max_sequence_length:\n",
    "            available_seq_len = (\n",
    "                self.max_sequence_length - conditional_tokens_len + len(context_tokens)\n",
    "            )\n",
    "            context_tokens = context_tokens[:available_seq_len]\n",
    "            \n",
    "            \n",
    "        tokens = [self.sos_token] + context_tokens + [self.eos_token]\n",
    "        \n",
    "        assert len(tokens) <= self.max_sequence_length\n",
    "        #tokens = tokens + [3] * (self.max_sequence_length - len(tokens))\n",
    "        return torch.tensor(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TqdmLoggingHandler(logging.Handler):\n",
    "    def __init__(self, level=logging.NOTSET):\n",
    "        super().__init__(level)\n",
    "\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            msg = self.format(record)\n",
    "            tqdm.tqdm.write(msg)\n",
    "            self.flush()\n",
    "        except (KeyboardInterrupt, SystemExit):\n",
    "            raise\n",
    "        except Exception:\n",
    "            self.handleError(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_logger(output_dir: str):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"[%(asctime)s] %(message)s\")\n",
    "\n",
    "    file_handler = logging.FileHandler(os.path.join(output_dir, \"train.log\"))\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    handler = TqdmLoggingHandler()\n",
    "    handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\"))\n",
    "    logger.addHandler(handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = []\n",
    "with open('data/as_set.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for i in data['data']:\n",
    "    doc_list.append(i['doc'])\n",
    "doc_list = list(set(doc_list))\n",
    "\n",
    "logger = _create_logger(output_dir=\"hisotry_finetuning_outputs\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "history_dataset = HistoryDataset(doc_list, tokenizer)\n",
    "history_data_loader = DataLoader(history_dataset, batch_size=1, shuffle=True, pin_memory=True, collate_fn = lambda x :x)\n",
    "\n",
    "learning_rate = 1e-5\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('KoGPT-2 Transfer Learning Start')\n",
    "\n",
    "epochs=200\n",
    "loss_list_between_log_interval = []\n",
    "count = 0\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for data in tqdm_notebook(history_data_loader):\n",
    "        data = data[0]\n",
    "        optimizer.zero_grad()\n",
    "        data= data.to(device)\n",
    "    \n",
    "        outputs = model(data, labels=data)\n",
    "        loss, logits = outputs[:2]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_list_between_log_interval.append(outputs.loss.item())\n",
    "        \n",
    "        if count %100 ==0:\n",
    "            mean_loss = np.mean(loss_list_between_log_interval)\n",
    "            logging.info('epoch no.{} train no.{}  loss = {} perplexity = {}' . format(epoch, count+1, loss, math.exp(mean_loss)))\n",
    "\n",
    "        if (count >0 and count%1000==0):\n",
    "            state_dict = model.state_dict()\n",
    "            model_path = os.path.join(\"hisotry_finetuning_outputs\", f\"gpt2_step_{count}.pth\")\n",
    "            torch.save(state_dict, model_path)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KoGPT2\n",
      "가군은 장안성 축성을 담당한 관리로 알려져 있다.\n",
      "이번 전시는 지난해부터 추진해온 '장안읍성과 함께하는 장안의 역사문화유산' 사업의 일환으로 마련됐다.\n",
      "전시에는 ▲조선시대와 조선시대를 대표하는 인물들의 초상화와 사진, 영상 등 다양한 볼거리들이 선보인다.\n",
      "특히 조선 후기 문신인 이강(李康)의 초상화도 만날 수 있어 눈길을 끈다.\n",
      "또한 이번 전시회에서는 조선후기 문인화가인 김학(金鶴) 화백의 작품 30여점이 소개된다.\n",
      "김화백은 \"중국의 문인들이 남긴 유품들을 통해 우리 민족의 얼과 정신을 느낄수 있는 좋은 기회가 될 것\"이라고 말했다.</d> 서울시는 오는\n",
      "\n",
      "After History Fintuning: \n",
      "가군은 장안성 축성을 담당한 관리로 관직은 참군이었다. 조선 건국 초기의 명신으로서 이근행이 있었다.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "text = '가군은 장안성 축성을 담당한 관리로'\n",
    "\n",
    "#  Load KoGPT2 model\n",
    "kogpt_model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "input_ids = tokenizer.encode(text)\n",
    "gen_ids = kogpt_model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "kogpt2_generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
    "\n",
    "print(\"KoGPT2\")\n",
    "print(kogpt2_generated)\n",
    "print()\n",
    "\n",
    "# Load After History Finetuning model\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "model.load_state_dict(torch.load('hisotry_finetuning_outputs/gpt2_step_200000.pth', map_location=\"cpu\"))\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file='Tokenizer/tokenizer.json',\n",
    "            bos_token=BOS, eos_token=EOS, unk_token='<unk>', \n",
    "            pad_token=PAD, mask_token=MASK)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
    "\n",
    "print(\"After History Fintuning: \")\n",
    "print(generated)\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
