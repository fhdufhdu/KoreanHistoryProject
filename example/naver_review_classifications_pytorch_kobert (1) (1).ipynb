{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "id": "0qHxvX90Ixdo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2021-02-04 16:13:08--  https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.18\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/dl/374ftkec978br3d/ratings_train.txt [following]\n",
      "--2021-02-04 16:13:08--  https://www.dropbox.com/s/dl/374ftkec978br3d/ratings_train.txt\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucc99e3c21c6feb5cb46618c7fb5.dl.dropboxusercontent.com/cd/0/get/BISSpNfSW9KhVRp0oPDEvYmRoF9i-dJ6OWwtM0z_nLgdiMQJnD0-oTuD-KOLcGiWpDG4R8lk_iLJ3fnW-gAifMB3JsAmRk-DP2Xse0cr0cSWgNr-uuBXSc2Xcv5_6-JpsCY/file?dl=1# [following]\n",
      "--2021-02-04 16:13:09--  https://ucc99e3c21c6feb5cb46618c7fb5.dl.dropboxusercontent.com/cd/0/get/BISSpNfSW9KhVRp0oPDEvYmRoF9i-dJ6OWwtM0z_nLgdiMQJnD0-oTuD-KOLcGiWpDG4R8lk_iLJ3fnW-gAifMB3JsAmRk-DP2Xse0cr0cSWgNr-uuBXSc2Xcv5_6-JpsCY/file?dl=1\n",
      "Resolving ucc99e3c21c6feb5cb46618c7fb5.dl.dropboxusercontent.com (ucc99e3c21c6feb5cb46618c7fb5.dl.dropboxusercontent.com)... 162.125.82.15\n",
      "Connecting to ucc99e3c21c6feb5cb46618c7fb5.dl.dropboxusercontent.com (ucc99e3c21c6feb5cb46618c7fb5.dl.dropboxusercontent.com)|162.125.82.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14628807 (14M) [application/binary]\n",
      "Saving to: 'ratings_train.txt@dl=1'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0%  165K 86s\n",
      "    50K .......... .......... .......... .......... ..........  0%  721K 53s\n",
      "   100K .......... .......... .......... .......... ..........  1%  610K 43s\n",
      "   150K .......... .......... .......... .......... ..........  1%  592K 38s\n",
      "   200K .......... .......... .......... .......... ..........  1% 1019K 33s\n",
      "   250K .......... .......... .......... .......... ..........  2% 1.59M 29s\n",
      "   300K .......... .......... .......... .......... ..........  2% 1.70M 26s\n",
      "   350K .......... .......... .......... .......... ..........  2% 1.92M 23s\n",
      "   400K .......... .......... .......... .......... ..........  3% 2.55M 21s\n",
      "   450K .......... .......... .......... .......... ..........  3% 1.92M 20s\n",
      "   500K .......... .......... .......... .......... ..........  3% 2.00M 19s\n",
      "   550K .......... .......... .......... .......... ..........  4% 1.23M 18s\n",
      "   600K .......... .......... .......... .......... ..........  4% 2.25M 17s\n",
      "   650K .......... .......... .......... .......... ..........  4% 2.47M 16s\n",
      "   700K .......... .......... .......... .......... ..........  5%  325K 18s\n",
      "   750K .......... .......... .......... .......... ..........  5% 16.4M 17s\n",
      "   800K .......... .......... .......... .......... ..........  5% 28.6M 16s\n",
      "   850K .......... .......... .......... .......... ..........  6%  695K 16s\n",
      "   900K .......... .......... .......... .......... ..........  6% 22.7M 15s\n",
      "   950K .......... .......... .......... .......... ..........  6% 1.03M 15s\n",
      "  1000K .......... .......... .......... .......... ..........  7% 6.31M 14s\n",
      "  1050K .......... .......... .......... .......... ..........  7% 5.20M 13s\n",
      "  1100K .......... .......... .......... .......... ..........  8% 5.13M 13s\n",
      "  1150K .......... .......... .......... .......... ..........  8% 5.59M 12s\n",
      "  1200K .......... .......... .......... .......... ..........  8% 5.20M 12s\n",
      "  1250K .......... .......... .......... .......... ..........  9% 1.19M 12s\n",
      "  1300K .......... .......... .......... .......... ..........  9% 5.60M 11s\n",
      "  1350K .......... .......... .......... .......... ..........  9% 4.53M 11s\n",
      "  1400K .......... .......... .......... .......... .......... 10% 5.27M 11s\n",
      "  1450K .......... .......... .......... .......... .......... 10% 5.31M 10s\n",
      "  1500K .......... .......... .......... .......... .......... 10%  853K 11s\n",
      "  1550K .......... .......... .......... .......... .......... 11% 1.06M 11s\n",
      "  1600K .......... .......... .......... .......... .......... 11% 2.92M 10s\n",
      "  1650K .......... .......... .......... .......... .......... 11% 2.42M 10s\n",
      "  1700K .......... .......... .......... .......... .......... 12% 3.89M 10s\n",
      "  1750K .......... .......... .......... .......... .......... 12%  365K 11s\n",
      "  1800K .......... .......... .......... .......... .......... 12% 28.2M 10s\n",
      "  1850K .......... .......... .......... .......... .......... 13% 24.1M 10s\n",
      "  1900K .......... .......... .......... .......... .......... 13% 34.8M 10s\n",
      "  1950K .......... .......... .......... .......... .......... 13% 32.5M 9s\n",
      "  2000K .......... .......... .......... .......... .......... 14% 35.8M 9s\n",
      "  2050K .......... .......... .......... .......... .......... 14% 1.61M 9s\n",
      "  2100K .......... .......... .......... .......... .......... 15% 23.7M 9s\n",
      "  2150K .......... .......... .......... .......... .......... 15% 3.12M 9s\n",
      "  2200K .......... .......... .......... .......... .......... 15% 35.1M 8s\n",
      "  2250K .......... .......... .......... .......... .......... 16% 1.61M 8s\n",
      "  2300K .......... .......... .......... .......... .......... 16% 28.6M 8s\n",
      "  2350K .......... .......... .......... .......... .......... 16% 28.7M 8s\n",
      "  2400K .......... .......... .......... .......... .......... 17% 33.5M 8s\n",
      "  2450K .......... .......... .......... .......... .......... 17% 33.5M 8s\n",
      "  2500K .......... .......... .......... .......... .......... 17% 36.5M 7s\n",
      "  2550K .......... .......... .......... .......... .......... 18% 29.8M 7s\n",
      "  2600K .......... .......... .......... .......... .......... 18% 38.0M 7s\n",
      "  2650K .......... .......... .......... .......... .......... 18% 34.8M 7s\n",
      "  2700K .......... .......... .......... .......... .......... 19% 38.1M 7s\n",
      "  2750K .......... .......... .......... .......... .......... 19% 32.7M 7s\n",
      "  2800K .......... .......... .......... .......... .......... 19% 36.5M 7s\n",
      "  2850K .......... .......... .......... .......... .......... 20% 36.5M 6s\n",
      "  2900K .......... .......... .......... .......... .......... 20% 8.51M 6s\n",
      "  2950K .......... .......... .......... .......... .......... 20% 3.77M 6s\n",
      "  3000K .......... .......... .......... .......... .......... 21% 5.64M 6s\n",
      "  3050K .......... .......... .......... .......... .......... 21% 5.45M 6s\n",
      "  3100K .......... .......... .......... .......... .......... 22% 4.65M 6s\n",
      "  3150K .......... .......... .......... .......... .......... 22% 5.46M 6s\n",
      "  3200K .......... .......... .......... .......... .......... 22% 5.24M 6s\n",
      "  3250K .......... .......... .......... .......... .......... 23% 4.83M 6s\n",
      "  3300K .......... .......... .......... .......... .......... 23% 5.06M 6s\n",
      "  3350K .......... .......... .......... .......... .......... 23% 4.42M 6s\n",
      "  3400K .......... .......... .......... .......... .......... 24% 4.96M 5s\n",
      "  3450K .......... .......... .......... .......... .......... 24% 4.49M 5s\n",
      "  3500K .......... .......... .......... .......... .......... 24% 5.51M 5s\n",
      "  3550K .......... .......... .......... .......... .......... 25% 4.87M 5s\n",
      "  3600K .......... .......... .......... .......... .......... 25% 5.05M 5s\n",
      "  3650K .......... .......... .......... .......... .......... 25% 4.24M 5s\n",
      "  3700K .......... .......... .......... .......... .......... 26% 4.25M 5s\n",
      "  3750K .......... .......... .......... .......... .......... 26% 3.81M 5s\n",
      "  3800K .......... .......... .......... .......... .......... 26% 6.54M 5s\n",
      "  3850K .......... .......... .......... .......... .......... 27% 6.13M 5s\n",
      "  3900K .......... .......... .......... .......... .......... 27% 5.18M 5s\n",
      "  3950K .......... .......... .......... .......... .......... 27% 5.32M 5s\n",
      "  4000K .......... .......... .......... .......... .......... 28% 5.05M 5s\n",
      "  4050K .......... .......... .......... .......... .......... 28% 6.02M 5s\n",
      "  4100K .......... .......... .......... .......... .......... 29% 4.40M 5s\n",
      "  4150K .......... .......... .......... .......... .......... 29% 3.83M 5s\n",
      "  4200K .......... .......... .......... .......... .......... 29% 4.61M 4s\n",
      "  4250K .......... .......... .......... .......... .......... 30% 5.13M 4s\n",
      "  4300K .......... .......... .......... .......... .......... 30% 4.74M 4s\n",
      "  4350K .......... .......... .......... .......... .......... 30% 4.59M 4s\n",
      "  4400K .......... .......... .......... .......... .......... 31% 4.75M 4s\n",
      "  4450K .......... .......... .......... .......... .......... 31% 5.51M 4s\n",
      "  4500K .......... .......... .......... .......... .......... 31%  968K 4s\n",
      "  4550K .......... .......... .......... .......... .......... 32% 4.09M 4s\n",
      "  4600K .......... .......... .......... .......... .......... 32% 4.87M 4s\n",
      "  4650K .......... .......... .......... .......... .......... 32% 4.49M 4s\n",
      "  4700K .......... .......... .......... .......... .......... 33% 5.05M 4s\n",
      "  4750K .......... .......... .......... .......... .......... 33% 4.95M 4s\n",
      "  4800K .......... .......... .......... .......... .......... 33% 4.43M 4s\n",
      "  4850K .......... .......... .......... .......... .......... 34% 4.96M 4s\n",
      "  4900K .......... .......... .......... .......... .......... 34% 6.70M 4s\n",
      "  4950K .......... .......... .......... .......... .......... 34% 4.22M 4s\n",
      "  5000K .......... .......... .......... .......... .......... 35%  351K 4s\n",
      "  5050K .......... .......... .......... .......... .......... 35% 6.81M 4s\n",
      "  5100K .......... .......... .......... .......... .......... 36% 6.35M 4s\n",
      "  5150K .......... .......... .......... .......... .......... 36% 5.68M 4s\n",
      "  5200K .......... .......... .......... .......... .......... 36% 31.1M 4s\n",
      "  5250K .......... .......... .......... .......... .......... 37% 29.2M 4s\n",
      "  5300K .......... .......... .......... .......... .......... 37% 2.23M 4s\n",
      "  5350K .......... .......... .......... .......... .......... 37% 14.8M 4s\n",
      "  5400K .......... .......... .......... .......... .......... 38% 26.3M 4s\n",
      "  5450K .......... .......... .......... .......... .......... 38% 35.3M 4s\n",
      "  5500K .......... .......... .......... .......... .......... 38% 2.41M 4s\n",
      "  5550K .......... .......... .......... .......... .......... 39% 24.4M 4s\n",
      "  5600K .......... .......... .......... .......... .......... 39% 29.5M 4s\n",
      "  5650K .......... .......... .......... .......... .......... 39% 2.19M 4s\n",
      "  5700K .......... .......... .......... .......... .......... 40% 25.2M 3s\n",
      "  5750K .......... .......... .......... .......... .......... 40% 27.8M 3s\n",
      "  5800K .......... .......... .......... .......... .......... 40% 35.4M 3s\n",
      "  5850K .......... .......... .......... .......... .......... 41% 36.8M 3s\n",
      "  5900K .......... .......... .......... .......... .......... 41% 38.7M 3s\n",
      "  5950K .......... .......... .......... .......... .......... 41% 30.2M 3s\n",
      "  6000K .......... .......... .......... .......... .......... 42% 34.9M 3s\n",
      "  6050K .......... .......... .......... .......... .......... 42% 31.4M 3s\n",
      "  6100K .......... .......... .......... .......... .......... 43% 36.5M 3s\n",
      "  6150K .......... .......... .......... .......... .......... 43% 29.1M 3s\n",
      "  6200K .......... .......... .......... .......... .......... 43% 24.2M 3s\n",
      "  6250K .......... .......... .......... .......... .......... 44% 4.75M 3s\n",
      "  6300K .......... .......... .......... .......... .......... 44% 7.52M 3s\n",
      "  6350K .......... .......... .......... .......... .......... 44% 5.29M 3s\n",
      "  6400K .......... .......... .......... .......... .......... 45% 4.71M 3s\n",
      "  6450K .......... .......... .......... .......... .......... 45% 5.01M 3s\n",
      "  6500K .......... .......... .......... .......... .......... 45% 4.81M 3s\n",
      "  6550K .......... .......... .......... .......... .......... 46% 3.14M 3s\n",
      "  6600K .......... .......... .......... .......... .......... 46% 5.33M 3s\n",
      "  6650K .......... .......... .......... .......... .......... 46% 5.34M 3s\n",
      "  6700K .......... .......... .......... .......... .......... 47% 5.49M 3s\n",
      "  6750K .......... .......... .......... .......... .......... 47% 4.40M 3s\n",
      "  6800K .......... .......... .......... .......... .......... 47% 4.96M 3s\n",
      "  6850K .......... .......... .......... .......... .......... 48% 5.24M 3s\n",
      "  6900K .......... .......... .......... .......... .......... 48% 4.60M 3s\n",
      "  6950K .......... .......... .......... .......... .......... 48%  353K 3s\n",
      "  7000K .......... .......... .......... .......... .......... 49% 10.4M 3s\n",
      "  7050K .......... .......... .......... .......... .......... 49% 34.5M 3s\n",
      "  7100K .......... .......... .......... .......... .......... 50% 2.05M 3s\n",
      "  7150K .......... .......... .......... .......... .......... 50% 6.08M 3s\n",
      "  7200K .......... .......... .......... .......... .......... 50% 5.58M 3s\n",
      "  7250K .......... .......... .......... .......... .......... 51% 18.2M 3s\n",
      "  7300K .......... .......... .......... .......... .......... 51% 28.1M 3s\n",
      "  7350K .......... .......... .......... .......... .......... 51% 30.7M 2s\n",
      "  7400K .......... .......... .......... .......... .......... 52% 29.1M 2s\n",
      "  7450K .......... .......... .......... .......... .......... 52% 37.2M 2s\n",
      "  7500K .......... .......... .......... .......... .......... 52% 41.4M 2s\n",
      "  7550K .......... .......... .......... .......... .......... 53% 32.7M 2s\n",
      "  7600K .......... .......... .......... .......... .......... 53% 40.4M 2s\n",
      "  7650K .......... .......... .......... .......... .......... 53% 41.6M 2s\n",
      "  7700K .......... .......... .......... .......... .......... 54% 39.6M 2s\n",
      "  7750K .......... .......... .......... .......... .......... 54% 32.2M 2s\n",
      "  7800K .......... .......... .......... .......... .......... 54% 35.6M 2s\n",
      "  7850K .......... .......... .......... .......... .......... 55% 35.6M 2s\n",
      "  7900K .......... .......... .......... .......... .......... 55% 38.0M 2s\n",
      "  7950K .......... .......... .......... .......... .......... 55% 15.8M 2s\n",
      "  8000K .......... .......... .......... .......... .......... 56% 4.98M 2s\n",
      "  8050K .......... .......... .......... .......... .......... 56% 6.19M 2s\n",
      "  8100K .......... .......... .......... .......... .......... 57% 5.69M 2s\n",
      "  8150K .......... .......... .......... .......... .......... 57% 4.29M 2s\n",
      "  8200K .......... .......... .......... .......... .......... 57% 5.43M 2s\n",
      "  8250K .......... .......... .......... .......... .......... 58% 4.91M 2s\n",
      "  8300K .......... .......... .......... .......... .......... 58% 5.28M 2s\n",
      "  8350K .......... .......... .......... .......... .......... 58% 4.80M 2s\n",
      "  8400K .......... .......... .......... .......... .......... 59% 5.53M 2s\n",
      "  8450K .......... .......... .......... .......... .......... 59% 5.73M 2s\n",
      "  8500K .......... .......... .......... .......... .......... 59% 5.18M 2s\n",
      "  8550K .......... .......... .......... .......... .......... 60% 3.75M 2s\n",
      "  8600K .......... .......... .......... .......... .......... 60% 5.27M 2s\n",
      "  8650K .......... .......... .......... .......... .......... 60% 4.61M 2s\n",
      "  8700K .......... .......... .......... .......... .......... 61% 5.30M 2s\n",
      "  8750K .......... .......... .......... .......... .......... 61% 4.15M 2s\n",
      "  8800K .......... .......... .......... .......... .......... 61% 5.24M 2s\n",
      "  8850K .......... .......... .......... .......... .......... 62%  374K 2s\n",
      "  8900K .......... .......... .......... .......... .......... 62% 28.8M 2s\n",
      "  8950K .......... .......... .......... .......... .......... 62% 25.4M 2s\n",
      "  9000K .......... .......... .......... .......... .......... 63% 39.1M 2s\n",
      "  9050K .......... .......... .......... .......... .......... 63% 37.0M 2s\n",
      "  9100K .......... .......... .......... .......... .......... 64% 38.9M 2s\n",
      "  9150K .......... .......... .......... .......... .......... 64% 35.7M 2s\n",
      "  9200K .......... .......... .......... .......... .......... 64% 36.6M 2s\n",
      "  9250K .......... .......... .......... .......... .......... 65% 36.5M 2s\n",
      "  9300K .......... .......... .......... .......... .......... 65% 38.6M 2s\n",
      "  9350K .......... .......... .......... .......... .......... 65% 34.0M 2s\n",
      "  9400K .......... .......... .......... .......... .......... 66% 39.6M 2s\n",
      "  9450K .......... .......... .......... .......... .......... 66% 35.9M 2s\n",
      "  9500K .......... .......... .......... .......... .......... 66% 41.3M 1s\n",
      "  9550K .......... .......... .......... .......... .......... 67% 34.6M 1s\n",
      "  9600K .......... .......... .......... .......... .......... 67% 40.0M 1s\n",
      "  9650K .......... .......... .......... .......... .......... 67% 40.0M 1s\n",
      "  9700K .......... .......... .......... .......... .......... 68% 7.65M 1s\n",
      "  9750K .......... .......... .......... .......... .......... 68% 4.51M 1s\n",
      "  9800K .......... .......... .......... .......... .......... 68% 5.51M 1s\n",
      "  9850K .......... .......... .......... .......... .......... 69% 5.46M 1s\n",
      "  9900K .......... .......... .......... .......... .......... 69% 5.73M 1s\n",
      "  9950K .......... .......... .......... .......... .......... 69% 6.24M 1s\n",
      " 10000K .......... .......... .......... .......... .......... 70% 5.98M 1s\n",
      " 10050K .......... .......... .......... .......... .......... 70% 5.43M 1s\n",
      " 10100K .......... .......... .......... .......... .......... 71% 5.87M 1s\n",
      " 10150K .......... .......... .......... .......... .......... 71% 4.69M 1s\n",
      " 10200K .......... .......... .......... .......... .......... 71% 6.75M 1s\n",
      " 10250K .......... .......... .......... .......... .......... 72% 6.36M 1s\n",
      " 10300K .......... .......... .......... .......... .......... 72% 6.30M 1s\n",
      " 10350K .......... .......... .......... .......... .......... 72% 5.41M 1s\n",
      " 10400K .......... .......... .......... .......... .......... 73% 6.70M 1s\n",
      " 10450K .......... .......... .......... .......... .......... 73% 6.69M 1s\n",
      " 10500K .......... .......... .......... .......... .......... 73% 6.47M 1s\n",
      " 10550K .......... .......... .......... .......... .......... 74% 6.17M 1s\n",
      " 10600K .......... .......... .......... .......... .......... 74%  212K 1s\n",
      " 10650K .......... .......... .......... .......... .......... 74% 9.95M 1s\n",
      " 10700K .......... .......... .......... .......... .......... 75% 23.6M 1s\n",
      " 10750K .......... .......... .......... .......... .......... 75% 28.3M 1s\n",
      " 10800K .......... .......... .......... .......... .......... 75% 3.19M 1s\n",
      " 10850K .......... .......... .......... .......... .......... 76% 11.0M 1s\n",
      " 10900K .......... .......... .......... .......... .......... 76% 38.4M 1s\n",
      " 10950K .......... .......... .......... .......... .......... 76% 28.5M 1s\n",
      " 11000K .......... .......... .......... .......... .......... 77% 3.10M 1s\n",
      " 11050K .......... .......... .......... .......... .......... 77% 9.20M 1s\n",
      " 11100K .......... .......... .......... .......... .......... 78% 11.6M 1s\n",
      " 11150K .......... .......... .......... .......... .......... 78% 6.68M 1s\n",
      " 11200K .......... .......... .......... .......... .......... 78% 29.0M 1s\n",
      " 11250K .......... .......... .......... .......... .......... 79% 29.7M 1s\n",
      " 11300K .......... .......... .......... .......... .......... 79% 34.0M 1s\n",
      " 11350K .......... .......... .......... .......... .......... 79% 29.9M 1s\n",
      " 11400K .......... .......... .......... .......... .......... 80% 32.1M 1s\n",
      " 11450K .......... .......... .......... .......... .......... 80% 39.0M 1s\n",
      " 11500K .......... .......... .......... .......... .......... 80% 33.1M 1s\n",
      " 11550K .......... .......... .......... .......... .......... 81% 32.7M 1s\n",
      " 11600K .......... .......... .......... .......... .......... 81% 39.1M 1s\n",
      " 11650K .......... .......... .......... .......... .......... 81% 38.3M 1s\n",
      " 11700K .......... .......... .......... .......... .......... 82% 37.8M 1s\n",
      " 11750K .......... .......... .......... .......... .......... 82% 1.14M 1s\n",
      " 11800K .......... .......... .......... .......... .......... 82% 8.18M 1s\n",
      " 11850K .......... .......... .......... .......... .......... 83% 6.60M 1s\n",
      " 11900K .......... .......... .......... .......... .......... 83% 5.83M 1s\n",
      " 11950K .......... .......... .......... .......... .......... 83% 5.23M 1s\n",
      " 12000K .......... .......... .......... .......... .......... 84% 7.06M 1s\n",
      " 12050K .......... .......... .......... .......... .......... 84% 5.84M 1s\n",
      " 12100K .......... .......... .......... .......... .......... 85% 6.50M 1s\n",
      " 12150K .......... .......... .......... .......... .......... 85% 5.24M 1s\n",
      " 12200K .......... .......... .......... .......... .......... 85%  487K 1s\n",
      " 12250K .......... .......... .......... .......... .......... 86% 24.1M 1s\n",
      " 12300K .......... .......... .......... .......... .......... 86% 32.9M 1s\n",
      " 12350K .......... .......... .......... .......... .......... 86% 22.9M 1s\n",
      " 12400K .......... .......... .......... .......... .......... 87% 36.4M 1s\n",
      " 12450K .......... .......... .......... .......... .......... 87% 33.6M 1s\n",
      " 12500K .......... .......... .......... .......... .......... 87% 35.5M 1s\n",
      " 12550K .......... .......... .......... .......... .......... 88% 28.2M 0s\n",
      " 12600K .......... .......... .......... .......... .......... 88% 30.5M 0s\n",
      " 12650K .......... .......... .......... .......... .......... 88% 35.8M 0s\n",
      " 12700K .......... .......... .......... .......... .......... 89% 31.2M 0s\n",
      " 12750K .......... .......... .......... .......... .......... 89% 29.1M 0s\n",
      " 12800K .......... .......... .......... .......... .......... 89% 22.4M 0s\n",
      " 12850K .......... .......... .......... .......... .......... 90% 28.9M 0s\n",
      " 12900K .......... .......... .......... .......... .......... 90% 28.2M 0s\n",
      " 12950K .......... .......... .......... .......... .......... 90% 5.73M 0s\n",
      " 13000K .......... .......... .......... .......... .......... 91%  357K 0s\n",
      " 13050K .......... .......... .......... .......... .......... 91% 7.17M 0s\n",
      " 13100K .......... .......... .......... .......... .......... 92% 19.9M 0s\n",
      " 13150K .......... .......... .......... .......... .......... 92% 26.5M 0s\n",
      " 13200K .......... .......... .......... .......... .......... 92% 33.4M 0s\n",
      " 13250K .......... .......... .......... .......... .......... 93% 36.6M 0s\n",
      " 13300K .......... .......... .......... .......... .......... 93% 39.2M 0s\n",
      " 13350K .......... .......... .......... .......... .......... 93% 32.9M 0s\n",
      " 13400K .......... .......... .......... .......... .......... 94% 38.0M 0s\n",
      " 13450K .......... .......... .......... .......... .......... 94% 36.5M 0s\n",
      " 13500K .......... .......... .......... .......... .......... 94%  839K 0s\n",
      " 13550K .......... .......... .......... .......... .......... 95% 14.9M 0s\n",
      " 13600K .......... .......... .......... .......... .......... 95% 27.1M 0s\n",
      " 13650K .......... .......... .......... .......... .......... 95% 35.9M 0s\n",
      " 13700K .......... .......... .......... .......... .......... 96% 33.8M 0s\n",
      " 13750K .......... .......... .......... .......... .......... 96% 34.2M 0s\n",
      " 13800K .......... .......... .......... .......... .......... 96% 38.5M 0s\n",
      " 13850K .......... .......... .......... .......... .......... 97% 37.0M 0s\n",
      " 13900K .......... .......... .......... .......... .......... 97% 37.4M 0s\n",
      " 13950K .......... .......... .......... .......... .......... 97% 34.3M 0s\n",
      " 14000K .......... .......... .......... .......... .......... 98% 38.1M 0s\n",
      " 14050K .......... .......... .......... .......... .......... 98% 38.0M 0s\n",
      " 14100K .......... .......... .......... .......... .......... 99% 33.8M 0s\n",
      " 14150K .......... .......... .......... .......... .......... 99% 33.2M 0s\n",
      " 14200K .......... .......... .......... .......... .......... 99% 44.1M 0s\n",
      " 14250K .......... .......... .......... .....                100% 46.3M=4.0s\n",
      "\n",
      "2021-02-04 16:13:14 (3.52 MB/s) - 'ratings_train.txt@dl=1' saved [14628807/14628807]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2021-02-04 16:13:14--  https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.18\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/dl/977gbwh542gdy94/ratings_test.txt [following]\n",
      "--2021-02-04 16:13:14--  https://www.dropbox.com/s/dl/977gbwh542gdy94/ratings_test.txt\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc7267840cb4f7940a36cc3f70b6.dl.dropboxusercontent.com/cd/0/get/BIRFvFeXjFF4fk5fjmZbAIGUrm2suC9yier-oSjPhawY__u5-_MHmXA-57kE6QMPGEYVD7x3awy4pYa8DGd3X5pd362x_SoKRTS3QrHKXgoWi5aXTgqlT1eAbkAAekrnkr8/file?dl=1# [following]\n",
      "--2021-02-04 16:13:15--  https://uc7267840cb4f7940a36cc3f70b6.dl.dropboxusercontent.com/cd/0/get/BIRFvFeXjFF4fk5fjmZbAIGUrm2suC9yier-oSjPhawY__u5-_MHmXA-57kE6QMPGEYVD7x3awy4pYa8DGd3X5pd362x_SoKRTS3QrHKXgoWi5aXTgqlT1eAbkAAekrnkr8/file?dl=1\n",
      "Resolving uc7267840cb4f7940a36cc3f70b6.dl.dropboxusercontent.com (uc7267840cb4f7940a36cc3f70b6.dl.dropboxusercontent.com)... 162.125.82.15\n",
      "Connecting to uc7267840cb4f7940a36cc3f70b6.dl.dropboxusercontent.com (uc7267840cb4f7940a36cc3f70b6.dl.dropboxusercontent.com)|162.125.82.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4893335 (4.7M) [application/binary]\n",
      "Saving to: 'ratings_test.txt@dl=1'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1%  190K 25s\n",
      "    50K .......... .......... .......... .......... ..........  2%  317K 20s\n",
      "   100K .......... .......... .......... .......... ..........  3% 19.7M 13s\n",
      "   150K .......... .......... .......... .......... ..........  4%  563K 12s\n",
      "   200K .......... .......... .......... .......... ..........  5% 1.38M 10s\n",
      "   250K .......... .......... .......... .......... ..........  6% 1.93M 9s\n",
      "   300K .......... .......... .......... .......... ..........  7%  863K 8s\n",
      "   350K .......... .......... .......... .......... ..........  8% 1.87M 7s\n",
      "   400K .......... .......... .......... .......... ..........  9% 2.06M 7s\n",
      "   450K .......... .......... .......... .......... .......... 10% 2.55M 6s\n",
      "   500K .......... .......... .......... .......... .......... 11% 3.39M 5s\n",
      "   550K .......... .......... .......... .......... .......... 12% 1.36M 5s\n",
      "   600K .......... .......... .......... .......... .......... 13% 1.96M 5s\n",
      "   650K .......... .......... .......... .......... .......... 14% 1.96M 5s\n",
      "   700K .......... .......... .......... .......... .......... 15% 2.42M 4s\n",
      "   750K .......... .......... .......... .......... .......... 16% 3.15M 4s\n",
      "   800K .......... .......... .......... .......... .......... 17% 3.13M 4s\n",
      "   850K .......... .......... .......... .......... .......... 18% 1.15M 4s\n",
      "   900K .......... .......... .......... .......... .......... 19% 2.03M 4s\n",
      "   950K .......... .......... .......... .......... .......... 20% 1.81M 4s\n",
      "  1000K .......... .......... .......... .......... .......... 21% 2.96M 3s\n",
      "  1050K .......... .......... .......... .......... .......... 23% 6.85M 3s\n",
      "  1100K .......... .......... .......... .......... .......... 24% 7.51M 3s\n",
      "  1150K .......... .......... .......... .......... .......... 25% 6.35M 3s\n",
      "  1200K .......... .......... .......... .......... .......... 26% 6.38M 3s\n",
      "  1250K .......... .......... .......... .......... .......... 27% 5.55M 3s\n",
      "  1300K .......... .......... .......... .......... .......... 28% 6.68M 3s\n",
      "  1350K .......... .......... .......... .......... .......... 29% 4.27M 2s\n",
      "  1400K .......... .......... .......... .......... .......... 30% 2.53M 2s\n",
      "  1450K .......... .......... .......... .......... .......... 31% 5.07M 2s\n",
      "  1500K .......... .......... .......... .......... .......... 32% 5.74M 2s\n",
      "  1550K .......... .......... .......... .......... .......... 33% 6.70M 2s\n",
      "  1600K .......... .......... .......... .......... .......... 34% 7.16M 2s\n",
      "  1650K .......... .......... .......... .......... .......... 35% 5.90M 2s\n",
      "  1700K .......... .......... .......... .......... .......... 36% 5.70M 2s\n",
      "  1750K .......... .......... .......... .......... .......... 37% 5.29M 2s\n",
      "  1800K .......... .......... .......... .......... .......... 38% 6.26M 2s\n",
      "  1850K .......... .......... .......... .......... .......... 39% 8.24M 2s\n",
      "  1900K .......... .......... .......... .......... .......... 40% 8.69M 2s\n",
      "  1950K .......... .......... .......... .......... .......... 41% 8.83M 2s\n",
      "  2000K .......... .......... .......... .......... .......... 42% 9.22M 1s\n",
      "  2050K .......... .......... .......... .......... .......... 43% 11.4M 1s\n",
      "  2100K .......... .......... .......... .......... .......... 44% 9.54M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 46% 9.27M 1s\n",
      "  2200K .......... .......... .......... .......... .......... 47% 11.7M 1s\n",
      "  2250K .......... .......... .......... .......... .......... 48% 11.7M 1s\n",
      "  2300K .......... .......... .......... .......... .......... 49% 12.0M 1s\n",
      "  2350K .......... .......... .......... .......... .......... 50% 11.0M 1s\n",
      "  2400K .......... .......... .......... .......... .......... 51% 11.2M 1s\n",
      "  2450K .......... .......... .......... .......... .......... 52% 10.1M 1s\n",
      "  2500K .......... .......... .......... .......... .......... 53% 10.6M 1s\n",
      "  2550K .......... .......... .......... .......... .......... 54% 9.38M 1s\n",
      "  2600K .......... .......... .......... .......... .......... 55% 11.4M 1s\n",
      "  2650K .......... .......... .......... .......... .......... 56% 11.5M 1s\n",
      "  2700K .......... .......... .......... .......... .......... 57% 12.2M 1s\n",
      "  2750K .......... .......... .......... .......... .......... 58% 10.9M 1s\n",
      "  2800K .......... .......... .......... .......... .......... 59% 11.8M 1s\n",
      "  2850K .......... .......... .......... .......... .......... 60% 11.9M 1s\n",
      "  2900K .......... .......... .......... .......... .......... 61% 11.6M 1s\n",
      "  2950K .......... .......... .......... .......... .......... 62% 9.26M 1s\n",
      "  3000K .......... .......... .......... .......... .......... 63% 11.8M 1s\n",
      "  3050K .......... .......... .......... .......... .......... 64% 11.7M 1s\n",
      "  3100K .......... .......... .......... .......... .......... 65% 12.1M 1s\n",
      "  3150K .......... .......... .......... .......... .......... 66% 10.7M 1s\n",
      "  3200K .......... .......... .......... .......... .......... 68% 11.8M 1s\n",
      "  3250K .......... .......... .......... .......... .......... 69% 12.1M 1s\n",
      "  3300K .......... .......... .......... .......... .......... 70% 10.8M 1s\n",
      "  3350K .......... .......... .......... .......... .......... 71% 9.88M 1s\n",
      "  3400K .......... .......... .......... .......... .......... 72% 11.7M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 73% 12.1M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 74% 11.6M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 75%  376K 0s\n",
      "  3600K .......... .......... .......... .......... .......... 76% 21.9M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 77% 29.8M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 78% 33.4M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 79% 6.80M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 80% 32.6M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 81% 34.9M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 82% 30.8M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 83% 4.48M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 84% 27.4M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 85% 15.7M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 86% 27.6M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 87% 14.6M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 88% 32.1M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 89% 21.4M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 91% 26.1M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 92% 17.5M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 93% 23.1M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 94%  674K 0s\n",
      "  4500K .......... .......... .......... .......... .......... 95% 12.6M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 96% 9.36M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 97% 12.2M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 98% 11.6M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 99% 11.5M 0s\n",
      "  4750K .......... .......... ........                        100% 14.9M=1.5s\n",
      "\n",
      "2021-02-04 16:13:17 (3.04 MB/s) - 'ratings_test.txt@dl=1' saved [4893335/4893335]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n",
    "!wget https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "SnIDhZFGIxdg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "  Using cached mxnet-1.7.0.post1-py2.py3-none-win_amd64.whl (33.0 MB)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Using cached graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting numpy<1.17.0,>=1.8.2\n",
      "  Using cached numpy-1.16.6-cp38-cp38-win_amd64.whl\n",
      "Collecting requests<2.19.0,>=2.18.4\n",
      "  Using cached requests-2.18.4-py2.py3-none-any.whl (88 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2020.12.5)\n",
      "Collecting chardet<3.1.0,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting idna<2.7,>=2.5\n",
      "  Using cached idna-2.6-py2.py3-none-any.whl (56 kB)\n",
      "Collecting urllib3<1.23,>=1.21.1\n",
      "  Using cached urllib3-1.22-py2.py3-none-any.whl (132 kB)\n",
      "Installing collected packages: urllib3, idna, chardet, requests, numpy, graphviz, mxnet\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "      Successfully uninstalled numpy-1.19.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] 액세스가 거부되었습니다: 'C:\\\\Users\\\\DeepLearning_4\\\\anaconda3\\\\envs\\\\pytorch1.7.1_p38\\\\Lib\\\\site-packages\\\\~umpy\\\\core\\\\_multiarray_tests.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gluonnlp in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: cython in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (from gluonnlp) (0.29.21)\n",
      "Requirement already satisfied: packaging in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (from gluonnlp) (20.8)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (from gluonnlp) (1.16.6)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.2.1-cp38-cp38-win_amd64.whl (9.3 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Collecting pytz>=2017.3\n",
      "  Using cached pytz-2020.5-py2.py3-none-any.whl (510 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.56.0-py2.py3-none-any.whl (72 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (from packaging->gluonnlp) (2.4.7)\n",
      "Installing collected packages: pytz, tqdm, pandas\n",
      "Successfully installed pandas-1.2.1 pytz-2020.5 tqdm-4.56.0\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.95-cp38-cp38-win_amd64.whl (1.2 MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.95\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.2.2-py3-none-any.whl (1.8 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (from transformers) (4.56.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (from transformers) (20.8)\n",
      "Requirement already satisfied: requests in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (from transformers) (2.18.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (from transformers) (1.16.6)\n",
      "Collecting tokenizers==0.9.4\n",
      "  Using cached tokenizers-0.9.4-cp38-cp38-win_amd64.whl (1.9 MB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.11.13-cp38-cp38-win_amd64.whl (270 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (from requests->transformers) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (from requests->transformers) (2.6)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.43-py3-none-any.whl\n",
      "Requirement already satisfied: six in c:\\users\\deeplearning_4\\anaconda3\\envs\\pytorch1.7.1_p38\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Collecting click\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.0.0-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: regex, joblib, click, tokenizers, sacremoses, filelock, transformers\n",
      "Successfully installed click-7.1.2 filelock-3.0.12 joblib-1.0.0 regex-2020.11.13 sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet\n",
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "spLEF0TBIxdm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to c:\\users\\deeplearning_4\\appdata\\local\\temp\\pip-req-build-narxdmi6\n",
      "Building wheels for collected packages: kobert\n",
      "  Building wheel for kobert (setup.py): started\n",
      "  Building wheel for kobert (setup.py): finished with status 'done'\n",
      "  Created wheel for kobert: filename=kobert-0.1.2-py3-none-any.whl size=12795 sha256=c8f302c36a9b949576e82e54e94e96b051e1d7e44dbe20b1cde43ea1dca1ae63\n",
      "  Stored in directory: C:\\Users\\DeepLearning_4\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-dgbwfgq0\\wheels\\bf\\5f\\74\\81bf3a1332130eb6629ecf58876a8746b77021e7d7b0638e91\n",
      "Successfully built kobert\n",
      "Installing collected packages: kobert\n",
      "Successfully installed kobert-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " D 드라이브의 볼륨: 새 볼륨\n",
      " 볼륨 일련 번호: 2070-7450\n",
      "\n",
      " D:\\sig2021S 디렉터리\n",
      "\n",
      "2021-02-03  오전 01:15    <DIR>          .\n",
      "2021-02-03  오전 01:15    <DIR>          ..\n",
      "2021-02-03  오전 12:48    <DIR>          .ipynb_checkpoints\n",
      "2021-01-19  오후 12:58         1,506,259 04_(개정)케라스로_Q&A_구현하기_With_SQUAD.ipynb\n",
      "2021-01-27  오후 03:49         1,536,900 05_케라스로_KorQuAD(한국어_Q&A)_구현하기.ipynb\n",
      "2021-02-02  오후 04:47            13,812 1.4.0\n",
      "2021-01-18  오후 07:34    <DIR>          bert\n",
      "2021-01-18  오후 07:34         4,854,279 dev-v1.1.json\n",
      "2021-01-29  오후 10:37    <DIR>          KoBERT\n",
      "2021-01-19  오후 01:21         3,881,058 KorQuAD_v1.0_dev.json\n",
      "2021-01-19  오후 01:21        38,527,475 KorQuAD_v1.0_train.json\n",
      "2021-01-19  오후 03:01       708,923,464 korquad_wordpiece.h5\n",
      "2021-01-19  오후 06:16       708,923,464 korquad_wordpiece_2.h5\n",
      "2021-01-19  오후 08:44       708,923,464 korquad_wordpiece_3.h5\n",
      "2018-11-24  오전 09:57       662,903,077 multi_cased_L-12_H-768_A-12.zip\n",
      "2018-11-24  오전 09:57       662,903,077 multi_cased_L-12_H-768_A-12.zip.1\n",
      "2021-01-23  오후 07:17        14,704,145 naver_it.xlsx\n",
      "2021-01-24  오후 03:03        14,437,752 naver_it2.xlsx\n",
      "2021-02-03  오전 01:15           135,776 naver_review_classifications_pytorch_kobert (1).ipynb\n",
      "2021-01-27  오후 04:04            10,474 naver_review_classifications_pytorch_kobert (2).ipynb\n",
      "2021-01-23  오후 07:07         1,835,524 naver_tech_it.xlsx\n",
      "2021-01-23  오후 01:24         8,235,455 naver_tech_it2.xlsx\n",
      "2021-01-23  오후 06:05         2,574,102 naver_tech_it3.xlsx\n",
      "2021-01-23  오후 06:53         1,404,998 naver_tech_it4.xlsx\n",
      "2021-01-27  오후 04:18         4,893,335 ratings_test.txt@dl=1\n",
      "2021-01-27  오후 04:23         4,893,335 ratings_test.txt@dl=1.1\n",
      "2021-01-27  오후 04:18        14,628,807 ratings_train.txt@dl=1\n",
      "2021-01-27  오후 04:23        14,628,807 ratings_train.txt@dl=1.1\n",
      "2021-01-18  오후 10:21       708,923,464 squad_wordpiece.h5\n",
      "2021-01-19  오전 03:02       708,923,464 squad_wordpiece_2.h5\n",
      "2021-01-19  오전 03:02       708,923,464 squad_wordpiece_3.h5\n",
      "2021-01-18  오후 07:34        30,288,272 train-v1.1.json\n",
      "2021-01-29  오후 11:25             6,550 Untitled.ipynb\n",
      "2021-02-03  오전 12:48                72 Untitled1.ipynb\n",
      "2021-01-23  오후 06:55            37,485 네이버 지식 백과 크롤러.ipynb\n",
      "2021-01-24  오후 09:19             4,540 문서 검색.ipynb\n",
      "2021-01-24  오후 07:51             8,804 엑셀 파일.ipynb\n",
      "2021-01-22  오후 10:04            40,567 웹 크롤러.ipynb\n",
      "2021-02-01  오후 11:24             2,875 주체 찾기.ipynb\n",
      "2021-02-01  오후 07:45            45,853 키워드 추출.ipynb\n",
      "2021-01-24  오후 07:57             1,019 테스ㅡ.ipynb\n",
      "              36개 파일       5,742,485,268 바이트\n",
      "               5개 디렉터리  1,828,743,544,832 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\sig2021S\\KoBERT\n"
     ]
    }
   ],
   "source": [
    "cd KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gluonnlp            0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip list | findstr glu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fHIxWwgmIxdn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Yb4CPgRXIxdo"
   },
   "outputs": [],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\"ratings_train.txt@dl=1\", field_indices=[1,2], num_discard_samples=1)\n",
    "dataset_test = nlp.data.TSVDataset(\"ratings_test.txt@dl=1\", field_indices=[1,2], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LFo26ZFdIxdo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WGfyy9J8Ixdp"
   },
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "cWGaS78DIxdq"
   },
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "I7j0L3qiIxdq"
   },
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "kTyTYoPoIxdq"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "w5GS_2wyIxdr"
   },
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "F9q7B95gIxdr"
   },
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jp5P8AF1Ixdr"
   },
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "wxaoJzTQIxdr"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer_grouped_parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f9761b30907a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer_grouped_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimizer_grouped_parameters' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5z_9QdaIxdr"
   },
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMQ8fjJiIxds"
   },
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCekJyTdIxds"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "06vZc1rMIxds"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 1/2344 [00:01<39:42,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.7608345746994019 train acc 0.484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████▊                                                                        | 201/2344 [01:10<12:33,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 201 loss 0.4502096474170685 train acc 0.5760261194029851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▌                                                                 | 401/2344 [02:20<11:19,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 401 loss 0.4633370339870453 train acc 0.6850841645885287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████▎                                                          | 601/2344 [03:31<10:19,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 601 loss 0.47878313064575195 train acc 0.7352329450915142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████████████▉                                                    | 801/2344 [04:41<09:01,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 801 loss 0.3990609645843506 train acc 0.7635962858926342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|█████████████████████████████████▎                                            | 1001/2344 [05:52<07:52,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1001 loss 0.3565847873687744 train acc 0.7806568431568431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|███████████████████████████████████████▉                                      | 1201/2344 [07:02<06:46,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1201 loss 0.31352269649505615 train acc 0.7929720024979184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████▌                               | 1401/2344 [08:13<05:35,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1401 loss 0.367765873670578 train acc 0.8017933618843683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████████████████████████████████▎                        | 1601/2344 [09:24<04:24,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1601 loss 0.3951683044433594 train acc 0.8096794971892567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████████████████████████████████████████████████████████▉                  | 1801/2344 [10:34<03:10,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1801 loss 0.2793704569339752 train acc 0.8161524847307051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████████████████████████████████████████████████▌           | 2001/2344 [11:45<02:01,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 2001 loss 0.3862469494342804 train acc 0.8218703148425787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████▏    | 2201/2344 [12:56<00:51,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 2201 loss 0.298382967710495 train acc 0.826875567923671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2344/2344 [13:46<00:00,  2.84it/s]\n",
      "  0%|                                                                                  | 1/782 [00:00<01:26,  9.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train acc 0.8300625711035268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 782/782 [01:29<00:00,  8.79it/s]\n",
      "  0%|                                                                                         | 0/2344 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.8728220907928389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                 | 1/2344 [00:00<14:02,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.49477285146713257 train acc 0.78125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████▊                                                                        | 201/2344 [01:11<12:48,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 201 loss 0.23916923999786377 train acc 0.8764769900497512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▌                                                                 | 401/2344 [02:21<11:26,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 401 loss 0.29475876688957214 train acc 0.8807668329177057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████▎                                                          | 601/2344 [03:32<10:19,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 601 loss 0.33969226479530334 train acc 0.8849573627287853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████████████▉                                                    | 801/2344 [04:43<09:08,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 801 loss 0.31565308570861816 train acc 0.886938202247191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|█████████████████████████████████▎                                            | 1001/2344 [05:54<07:59,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1001 loss 0.34469369053840637 train acc 0.8893138111888111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|███████████████████████████████████████▉                                      | 1201/2344 [07:05<06:45,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1201 loss 0.28234586119651794 train acc 0.8914706494587843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████▌                               | 1401/2344 [08:16<05:31,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1401 loss 0.2360248863697052 train acc 0.8935247144896502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████████████████████████████████▎                        | 1601/2344 [09:26<04:21,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1601 loss 0.2849579453468323 train acc 0.8958365865084322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████████████████████████████████████████████████████████▉                  | 1801/2344 [10:37<03:12,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1801 loss 0.17394313216209412 train acc 0.8975742642976124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████████████████████████████████████████████████▌           | 2001/2344 [11:48<02:00,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 2001 loss 0.31509140133857727 train acc 0.8995814592703648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████▏    | 2201/2344 [12:59<00:50,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 2201 loss 0.20486846566200256 train acc 0.9009683098591549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2344/2344 [13:49<00:00,  2.83it/s]\n",
      "  0%|                                                                                  | 1/782 [00:00<01:25,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train acc 0.9021415493458474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 782/782 [01:28<00:00,  8.79it/s]\n",
      "  0%|                                                                                         | 0/2344 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.8895460358056266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                 | 1/2344 [00:00<14:02,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.40498873591423035 train acc 0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████▊                                                                        | 201/2344 [01:11<12:36,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 201 loss 0.13944776356220245 train acc 0.9228855721393034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▌                                                                 | 401/2344 [02:22<11:26,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 401 loss 0.2124568074941635 train acc 0.9242908354114713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████▎                                                          | 601/2344 [03:32<10:16,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 601 loss 0.22336262464523315 train acc 0.9273086522462562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████████████▉                                                    | 801/2344 [04:43<09:07,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 801 loss 0.17711281776428223 train acc 0.9286243757802747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|█████████████████████████████████▎                                            | 1001/2344 [05:54<07:50,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1001 loss 0.2565745711326599 train acc 0.9311781968031968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|███████████████████████████████████████▉                                      | 1201/2344 [07:05<06:44,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1201 loss 0.1400611251592636 train acc 0.933479912572856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████▌                               | 1401/2344 [08:15<05:36,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1401 loss 0.08247078210115433 train acc 0.935012937187723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████████████████████████████████▎                        | 1601/2344 [09:26<04:22,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1601 loss 0.15432250499725342 train acc 0.935831121174266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████████████████████████████████████████████████████████▉                  | 1801/2344 [10:37<03:12,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1801 loss 0.09062138944864273 train acc 0.9371529705719045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████████████████████████████████████████████████▌           | 2001/2344 [11:48<02:03,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 2001 loss 0.19246870279312134 train acc 0.9382730509745127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████▏    | 2201/2344 [12:59<00:50,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 2201 loss 0.1389140784740448 train acc 0.93914697864607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2344/2344 [13:49<00:00,  2.83it/s]\n",
      "  0%|                                                                                  | 1/782 [00:00<01:25,  9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 train acc 0.9400952787258249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 782/782 [01:28<00:00,  8.79it/s]\n",
      "  0%|                                                                                         | 0/2344 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.8945212595907929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                 | 1/2344 [00:00<14:02,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.3146041929721832 train acc 0.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████▊                                                                        | 201/2344 [01:11<12:45,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 201 loss 0.07286043465137482 train acc 0.9547574626865671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▌                                                                 | 401/2344 [02:22<11:29,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 401 loss 0.12412142753601074 train acc 0.9551901496259352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████▎                                                          | 601/2344 [03:32<10:16,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 601 loss 0.14823992550373077 train acc 0.9571547420965059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████████████▉                                                    | 801/2344 [04:43<09:07,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 801 loss 0.1585911363363266 train acc 0.9577481273408239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|█████████████████████████████████▎                                            | 1001/2344 [05:54<07:56,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1001 loss 0.21702264249324799 train acc 0.9594155844155844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|███████████████████████████████████████▉                                      | 1201/2344 [07:05<06:38,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1201 loss 0.12065229564905167 train acc 0.9606057452123231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████▌                               | 1401/2344 [08:15<05:34,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1401 loss 0.033691927790641785 train acc 0.9614337972876517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████████████████████████████████▎                        | 1601/2344 [09:27<04:23,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1601 loss 0.014754309318959713 train acc 0.9620354465958776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████████████████████████████████████████████████████████▉                  | 1801/2344 [10:38<03:13,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1801 loss 0.024783875793218613 train acc 0.9630153387007219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████████████████████████████████████████████████▌           | 2001/2344 [11:49<02:00,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 2001 loss 0.06320244818925858 train acc 0.9638305847076462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████▏    | 2201/2344 [12:59<00:50,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 2201 loss 0.03596195951104164 train acc 0.9643201953657429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2344/2344 [13:50<00:00,  2.82it/s]\n",
      "  0%|                                                                                  | 1/782 [00:00<01:25,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 train acc 0.9648170861774744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 782/782 [01:28<00:00,  8.79it/s]\n",
      "  0%|                                                                                         | 0/2344 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.8968390345268542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                 | 1/2344 [00:00<14:02,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.23279397189617157 train acc 0.921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████▊                                                                        | 201/2344 [01:11<12:32,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 201 loss 0.04940285161137581 train acc 0.9720926616915423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▌                                                                 | 401/2344 [02:21<11:30,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 401 loss 0.07013072073459625 train acc 0.9736596009975063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████▎                                                          | 601/2344 [03:32<10:17,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 601 loss 0.11671853810548782 train acc 0.9738196755407654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████████████▉                                                    | 801/2344 [04:43<09:07,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 801 loss 0.04175371676683426 train acc 0.9746605805243446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|█████████████████████████████████▎                                            | 1001/2344 [05:53<08:01,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1001 loss 0.06888998299837112 train acc 0.975586913086913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|███████████████████████████████████████▉                                      | 1201/2344 [07:04<06:40,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1201 loss 0.022440576925873756 train acc 0.9761136552872606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████▌                               | 1401/2344 [08:15<05:31,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1401 loss 0.01562100276350975 train acc 0.9763338686652391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████████████████████████████████▎                        | 1601/2344 [09:25<04:22,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1601 loss 0.006175382062792778 train acc 0.9766552154903185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████████████████████████████████████████████████████████▉                  | 1801/2344 [10:36<03:10,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1801 loss 0.005685278680175543 train acc 0.9769919489172681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████████████████████████████████████████████████▌           | 2001/2344 [11:46<02:01,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 2001 loss 0.015749206766486168 train acc 0.9775268615692154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████▏    | 2201/2344 [12:57<00:50,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 2201 loss 0.020741073414683342 train acc 0.9775812130849614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2344/2344 [13:47<00:00,  2.83it/s]\n",
      "  0%|                                                                                  | 1/782 [00:00<01:25,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 train acc 0.9778556953924915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 782/782 [01:28<00:00,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.8964593989769821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward(torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁정말', '▁재미있', '어요']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok(\"정말 재미있어요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataLoader in module torch.utils.data.dataloader object:\n",
      "\n",
      "class DataLoader(typing.Generic)\n",
      " |  Data loader. Combines a dataset and a sampler, and provides an iterable over\n",
      " |  the given dataset.\n",
      " |  \n",
      " |  The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
      " |  iterable-style datasets with single- or multi-process loading, customizing\n",
      " |  loading order and optional automatic batching (collation) and memory pinning.\n",
      " |  \n",
      " |  See :py:mod:`torch.utils.data` documentation page for more details.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      dataset (Dataset): dataset from which to load the data.\n",
      " |      batch_size (int, optional): how many samples per batch to load\n",
      " |          (default: ``1``).\n",
      " |      shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
      " |          at every epoch (default: ``False``).\n",
      " |      sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
      " |          samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
      " |          implemented. If specified, :attr:`shuffle` must not be specified.\n",
      " |      batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
      " |          returns a batch of indices at a time. Mutually exclusive with\n",
      " |          :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
      " |          and :attr:`drop_last`.\n",
      " |      num_workers (int, optional): how many subprocesses to use for data\n",
      " |          loading. ``0`` means that the data will be loaded in the main process.\n",
      " |          (default: ``0``)\n",
      " |      collate_fn (callable, optional): merges a list of samples to form a\n",
      " |          mini-batch of Tensor(s).  Used when using batched loading from a\n",
      " |          map-style dataset.\n",
      " |      pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
      " |          into CUDA pinned memory before returning them.  If your data elements\n",
      " |          are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
      " |          see the example below.\n",
      " |      drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
      " |          if the dataset size is not divisible by the batch size. If ``False`` and\n",
      " |          the size of dataset is not divisible by the batch size, then the last batch\n",
      " |          will be smaller. (default: ``False``)\n",
      " |      timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
      " |          from workers. Should always be non-negative. (default: ``0``)\n",
      " |      worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
      " |          worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
      " |          input, after seeding and before data loading. (default: ``None``)\n",
      " |      prefetch_factor (int, optional, keyword-only arg): Number of sample loaded\n",
      " |          in advance by each worker. ``2`` means there will be a total of\n",
      " |          2 * num_workers samples prefetched across all workers. (default: ``2``)\n",
      " |      persistent_workers (bool, optional): If ``True``, the data loader will not shutdown\n",
      " |          the worker processes after a dataset has been consumed once. This allows to \n",
      " |          maintain the workers `Dataset` instances alive. (default: ``False``)\n",
      " |  \n",
      " |  \n",
      " |  .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
      " |               cannot be an unpicklable object, e.g., a lambda function. See\n",
      " |               :ref:`multiprocessing-best-practices` on more details related\n",
      " |               to multiprocessing in PyTorch.\n",
      " |  \n",
      " |  .. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
      " |               When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
      " |               it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
      " |               rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
      " |               configurations. This represents the best guess PyTorch can make because PyTorch\n",
      " |               trusts user :attr:`dataset` code in correctly handling multi-process\n",
      " |               loading to avoid duplicate data.\n",
      " |  \n",
      " |               However, if sharding results in multiple workers having incomplete last batches,\n",
      " |               this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
      " |               be broken into multiple ones and (2) more than one batch worth of samples can be\n",
      " |               dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
      " |               cases in general.\n",
      " |  \n",
      " |               See `Dataset Types`_ for more details on these two types of datasets and how\n",
      " |               :class:`~torch.utils.data.IterableDataset` interacts with\n",
      " |               `Multi-process data loading`_.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataLoader\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, dataset:torch.utils.data.dataset.Dataset, batch_size:Union[int, NoneType]=1, shuffle:bool=False, sampler:Union[torch.utils.data.sampler.Sampler[int], NoneType]=None, batch_sampler:Union[torch.utils.data.sampler.Sampler[Sequence[int]], NoneType]=None, num_workers:int=0, collate_fn:Callable[[List[~T]], Any]=None, pin_memory:bool=False, drop_last:bool=False, timeout:float=0, worker_init_fn:Callable[[int], NoneType]=None, multiprocessing_context=None, generator=None, *, prefetch_factor:int=2, persistent_workers:bool=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self) -> '_BaseDataLoaderIter'\n",
      " |      # We quote '_BaseDataLoaderIter' since it isn't defined yet and the definition can't be moved up\n",
      " |      # since '_BaseDataLoaderIter' references 'DataLoader'.\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |  \n",
      " |  __setattr__(self, attr, val)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  multiprocessing_context\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_iterator': typing.Union[_ForwardRef('_BaseDataLoa...\n",
      " |  \n",
      " |  __args__ = None\n",
      " |  \n",
      " |  __extra__ = None\n",
      " |  \n",
      " |  __next_in_mro__ = <class 'object'>\n",
      " |      The most base type\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  __origin__ = None\n",
      " |  \n",
      " |  __parameters__ = (+T_co,)\n",
      " |  \n",
      " |  __tree_hash__ = -9223371885817198994\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwds)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-fffeb8033141>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "(token_ids, valid_length, segment_ids, label) = train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-bb8303fdecb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtoken_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msegment_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtoken_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected np.ndarray (got list)"
     ]
    }
   ],
   "source": [
    "data = data_train.__getitem__(0)\n",
    "token_ids = torch.from_numpy(data[0])\n",
    "segment_ids = torch.from_numpy(data[2])\n",
    "token_data = torch.from_numpy(tok(dataset_train.__getitem__(0)[0]))\n",
    "\n",
    "print(token_ids)\n",
    "print(token_data)\n",
    "print(segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zeros_like(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-6586b4a11188>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-9e25613a1eab>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, token_ids, valid_length, segment_ids)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mattention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen_attention_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpooler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-9e25613a1eab>\u001b[0m in \u001b[0;36mgen_attention_mask\u001b[1;34m(self, token_ids, valid_length)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgen_attention_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mattention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: zeros_like(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "model(token_data, token_ids, segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"naverClass\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "naver_review_classifications_pytorch_kobert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch1.7.1",
   "language": "python",
   "name": "torch1.7.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}