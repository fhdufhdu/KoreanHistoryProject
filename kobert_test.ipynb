{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('sig': conda)"
  },
  "interpreter": {
   "hash": "fa77a008a21e832a8c4c5192ebdf5151ad890f56fb8c2da17132c8e04b4b7627"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch==1.7.1/transformers==3.5.1\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import codecs\n",
    "\n",
    "from gluonnlp.data import SentencepieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_json_to_dataframe_train(input_file_path, record_path = ['data','paragraphs','qas','answers'], verbose = 1):\n",
    "    if verbose:\n",
    "        print(\"Reading the json file\")    \n",
    "    file = json.loads(open(input_file_path).read())\n",
    "    if verbose:\n",
    "        print(\"processing...\")\n",
    "    # parsing different level's in the json file\n",
    "    js = pd.io.json.json_normalize(file , record_path )\n",
    "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
    "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
    "    \n",
    "    #combining it into single dataframe\n",
    "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
    "    ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
    "    m['context'] = idx\n",
    "    js['q_idx'] = ndx\n",
    "    main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n",
    "    main['c_id'] = main['context'].factorize()[0]\n",
    "    if verbose:\n",
    "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
    "        print(\"Done\")\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading the json file\n",
      "processing...\n",
      "<ipython-input-3-47c561302d9e>:8: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  js = pd.io.json.json_normalize(file , record_path )\n",
      "<ipython-input-3-47c561302d9e>:9: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
      "<ipython-input-3-47c561302d9e>:10: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  r = pd.io.json.json_normalize(file,record_path[:-2])\n",
      "<ipython-input-3-47c561302d9e>:17: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n",
      "shape of the dataframe is (60407, 6)\n",
      "Done\n",
      "Reading the json file\n",
      "processing...\n",
      "shape of the dataframe is (5774, 6)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_raw = squad_json_to_dataframe_train(\"KorQuAD_v1.0_train.json\")\n",
    "test_raw = squad_json_to_dataframe_train(\"KorQuAD_v1.0_dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "\n",
    "#token_idx to token_word\n",
    "#print(vocab.idx_to_token[3460])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, bert_tokenizer, max_len,\n",
    "                 pad=True, pair=True):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "        self.max = 0\n",
    "        questions = list(dataset['question'])\n",
    "        contexts = list(dataset['context'])\n",
    "\n",
    "        labels_temp = list(dataset['answer_start'])\n",
    "        # self.sentences = []\n",
    "        # for q, c in zip(questions, contexts):\n",
    "        #     len_ = len(bert_tokenizer(q)) + len(bert_tokenizer(c))\n",
    "        #     if self.max < len_:\n",
    "        #         self.max = len_\n",
    "        #         print(self.max)\n",
    "        #     self.sentences.append(transform([q, c]))\n",
    "        self.sentences = [transform([q, c]) for q, c in zip(questions, contexts)]\n",
    "        self.labels = [np.int32(label) for label in labels_temp]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "PAD = True\n",
    "PAIR = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "408\n",
      "458\n",
      "502\n",
      "640\n",
      "701\n",
      "707\n",
      "1002\n",
      "1009\n",
      "1150\n",
      "1598\n",
      "1604\n",
      "1618\n",
      "6712\n",
      "6717\n",
      "6718\n",
      "6721\n",
      "291\n",
      "292\n",
      "295\n",
      "296\n",
      "421\n",
      "588\n",
      "596\n",
      "601\n",
      "611\n",
      "698\n",
      "1272\n",
      "1274\n",
      "1278\n",
      "1444\n",
      "1449\n",
      "1451\n",
      "1453\n",
      "1715\n",
      "1723\n",
      "1725\n",
      "6721\n",
      "1725\n"
     ]
    }
   ],
   "source": [
    "data_train = BERTDataset(train_raw,tok,MAX_LEN,PAD,PAIR)\n",
    "data_test = BERTDataset(test_raw,tok,MAX_LEN,PAD,PAIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_QA_model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}